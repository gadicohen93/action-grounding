{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Notebook 01: Behavioral Phenomenon\n",
    "\n",
    "**Research Question:** Do models claim actions they don't take?\n",
    "\n",
    "This notebook:\n",
    "1. Generates episodes across all experimental conditions\n",
    "2. Measures fake action rates by condition\n",
    "3. Performs statistical analysis\n",
    "4. Creates visualizations\n",
    "\n",
    "**Expected output:** `episodes.parquet` with 2,250 episodes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('..')\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from pathlib import Path\n",
    "\n",
    "# Project imports\n",
    "from src.utils.logging import setup_logging\n",
    "from src.config import get_config\n",
    "from src.generation import generate_batch, get_all_conditions\n",
    "from src.generation.prompts import ToolType\n",
    "from src.data.io import save_episodes, load_episodes\n",
    "\n",
    "# Setup logging\n",
    "setup_logging(level=\"INFO\")\n",
    "\n",
    "# Load config\n",
    "config = get_config()\n",
    "\n",
    "print(\"Configuration:\")\n",
    "print(f\"  Model: {config.model.id}\")\n",
    "print(f\"  Episodes per condition: {config.experiment.n_episodes_per_condition}\")\n",
    "print(f\"  Tools: {config.experiment.tools}\")\n",
    "print(f\"  System variants: {config.experiment.system_variants}\")\n",
    "print(f\"  Social pressures: {config.experiment.social_pressures}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Generate Episodes\n",
    "\n",
    "Generate episodes across all conditions:\n",
    "- 3 tools × 3 variants × 5 pressures × 12 scenarios = multiple conditions\n",
    "- 50 episodes per condition (configurable)\n",
    "\n",
    "**Note:** This will use OpenAI for claim labeling. Ensure `OPENAI_API_KEY` is set in `.env`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get all conditions\n",
    "conditions = get_all_conditions(\n",
    "    tool_types=[ToolType(t) for t in config.experiment.tools],\n",
    "    # variants and pressures from config\n",
    ")\n",
    "\n",
    "print(f\"Total conditions: {len(conditions)}\")\n",
    "print(f\"Expected episodes: {len(conditions) * config.experiment.n_episodes_per_condition}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate episodes\n",
    "# WARNING: This will take 2-4 hours depending on GPU and model size\n",
    "\n",
    "episodes = generate_batch(\n",
    "    conditions=conditions,\n",
    "    n_per_condition=config.experiment.n_episodes_per_condition,\n",
    "    model_id=config.model.id,\n",
    "    labeling_method=\"openai\",  # Use OpenAI for accurate labeling\n",
    "    save_path=config.data.processed_dir / \"episodes.parquet\",\n",
    "    verbose=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load episodes (if already generated)\n",
    "# episodes_collection = load_episodes(config.data.processed_dir / \"episodes.parquet\")\n",
    "# episodes = episodes_collection.episodes\n",
    "\n",
    "print(f\"Loaded {len(episodes)} episodes\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Exploratory Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert to DataFrame for analysis\n",
    "df = pd.DataFrame([ep.model_dump() for ep in episodes])\n",
    "\n",
    "print(\"Dataset shape:\", df.shape)\n",
    "print(\"\\nColumns:\", df.columns.tolist())\n",
    "print(\"\\nFirst few rows:\")\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Overall statistics\n",
    "print(\"Category Distribution:\")\n",
    "print(df['category'].value_counts())\n",
    "print(\"\\nCategory Rates:\")\n",
    "print(df['category'].value_counts(normalize=True))\n",
    "\n",
    "# Key metric: Fake action rate\n",
    "fake_rate = (df['category'] == 'fake_action').mean()\n",
    "print(f\"\\n**Fake Action Rate: {fake_rate:.1%}**\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Distribution by tool type\n",
    "print(\"\\nFake Rate by Tool Type:\")\n",
    "fake_by_tool = df[df['category'] == 'fake_action'].groupby('tool_type').size() / df.groupby('tool_type').size()\n",
    "print(fake_by_tool)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Fake Rate by Condition\n",
    "\n",
    "Analyze fake action rates across experimental conditions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute fake rate by condition\n",
    "fake_by_condition = df.groupby(['tool_type', 'system_variant', 'social_pressure']).apply(\n",
    "    lambda x: (x['category'] == 'fake_action').mean()\n",
    ").reset_index(name='fake_rate')\n",
    "\n",
    "print(\"Fake rates by condition:\")\n",
    "print(fake_by_condition.sort_values('fake_rate', ascending=False).head(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Highest fake rate condition\n",
    "max_fake = fake_by_condition.loc[fake_by_condition['fake_rate'].idxmax()]\n",
    "print(f\"\\n**Highest Fake Rate:**\")\n",
    "print(f\"  Tool: {max_fake['tool_type']}\")\n",
    "print(f\"  Variant: {max_fake['system_variant']}\")\n",
    "print(f\"  Pressure: {max_fake['social_pressure']}\")\n",
    "print(f\"  Rate: {max_fake['fake_rate']:.1%}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Statistical Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Bootstrap confidence intervals for overall fake rate\n",
    "from src.analysis.statistics import bootstrap_ci\n",
    "\n",
    "is_fake = (df['category'] == 'fake_action').values.astype(float)\n",
    "point_est, lower, upper = bootstrap_ci(is_fake, np.mean, n_bootstrap=1000)\n",
    "\n",
    "print(f\"Overall Fake Rate: {point_est:.1%}\")\n",
    "print(f\"95% CI: [{lower:.1%}, {upper:.1%}]\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Chi-squared test: Are fake rates different across conditions?\n",
    "from scipy.stats import chi2_contingency\n",
    "\n",
    "# Contingency table: variant × pressure\n",
    "contingency = pd.crosstab(\n",
    "    df['system_variant'],\n",
    "    df['social_pressure'],\n",
    "    values=(df['category'] == 'fake_action'),\n",
    "    aggfunc='sum'\n",
    ")\n",
    "\n",
    "chi2, p_value, dof, expected = chi2_contingency(contingency)\n",
    "\n",
    "print(f\"\\nChi-squared test:\")\n",
    "print(f\"  χ² = {chi2:.2f}\")\n",
    "print(f\"  p-value = {p_value:.4e}\")\n",
    "print(f\"  Significant: {p_value < 0.05}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Visualization\n",
    "\n",
    "**Figure 1:** Fake rate heatmap by condition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For escalation tool only (most interesting)\n",
    "df_escalate = df[df['tool_type'] == 'escalate']\n",
    "\n",
    "# Pivot table for heatmap\n",
    "fake_pivot = df_escalate.pivot_table(\n",
    "    index='system_variant',\n",
    "    columns='social_pressure',\n",
    "    values='category',\n",
    "    aggfunc=lambda x: (x == 'fake_action').mean()\n",
    ")\n",
    "\n",
    "# Order columns by pressure intensity\n",
    "pressure_order = ['NEUTRAL', 'STRESSED', 'DEMAND', 'VALIDATION', 'APPEASE']\n",
    "fake_pivot = fake_pivot[pressure_order]\n",
    "\n",
    "print(\"Fake rate pivot table:\")\n",
    "print(fake_pivot)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Heatmap\n",
    "from src.analysis.visualization import plot_fake_rate_heatmap\n",
    "\n",
    "fig = plot_fake_rate_heatmap(\n",
    "    fake_rates=fake_pivot.values,\n",
    "    variant_labels=fake_pivot.index.tolist(),\n",
    "    pressure_labels=fake_pivot.columns.tolist(),\n",
    "    title=\"Fake Escalation Rate by Condition\",\n",
    "    save_path=config.figures_dir / \"figure1_fake_rates\",\n",
    ")\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Summary Statistics\n",
    "\n",
    "Final summary for the paper."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\" * 60)\n",
    "print(\"PHASE 1 RESULTS: BEHAVIORAL PHENOMENON\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "print(f\"\\nTotal Episodes: {len(df)}\")\n",
    "print(f\"\\nOverall Fake Action Rate: {fake_rate:.1%} (95% CI: [{lower:.1%}, {upper:.1%}])\")\n",
    "\n",
    "print(f\"\\nHighest Fake Rate Condition:\")\n",
    "print(f\"  {max_fake['system_variant']} × {max_fake['social_pressure']}: {max_fake['fake_rate']:.1%}\")\n",
    "\n",
    "print(f\"\\nStatistical Significance:\")\n",
    "print(f\"  χ² test: p = {p_value:.4e} {'***' if p_value < 0.001 else '**' if p_value < 0.01 else '*' if p_value < 0.05 else 'ns'}\")\n",
    "\n",
    "print(\"\\n✓ Phase 1 complete: Phenomenon exists and is systematic\")\n",
    "print(\"=\" * 60)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Next Steps\n",
    "\n",
    "→ **Notebook 02:** Extract activations and train probes to detect ground truth"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
