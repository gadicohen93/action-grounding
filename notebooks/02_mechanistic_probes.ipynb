{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Notebook 02: Mechanistic Probes\n",
    "\n",
    "**Research Question:** Can we detect ground truth action-taking from model activations?\n",
    "\n",
    "This notebook:\n",
    "1. Extracts activations at multiple positions and layers\n",
    "2. Trains reality (tool_used) and narrative (claims_action) probes\n",
    "3. **Critical:** Position analysis (first_assistant vs before_tool)\n",
    "4. Analyzes probe behavior on fake action cases\n",
    "\n",
    "**Key hypothesis:** If `first_assistant` accuracy > 80%, probe is detecting action-grounding, not just tool syntax."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('..')\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from pathlib import Path\n",
    "\n",
    "from src.utils.logging import setup_logging\n",
    "from src.config import get_config\n",
    "from src.data.io import load_episodes, save_activations, load_activations\n",
    "from src.extraction import extract_activations_batch\n",
    "from src.analysis.probes import train_and_evaluate, analyze_probe_on_category, save_probe\n",
    "from src.analysis.visualization import (\n",
    "    plot_confusion_matrix,\n",
    "    plot_roc_curve,\n",
    "    plot_position_accuracy,\n",
    "    plot_layer_analysis,\n",
    ")\n",
    "from src.analysis.statistics import compute_roc_auc, bootstrap_metrics\n",
    "\n",
    "setup_logging(level=\"INFO\")\n",
    "config = get_config()\n",
    "\n",
    "print(f\"Extraction config:\")\n",
    "print(f\"  Positions: {config.extraction.positions}\")\n",
    "print(f\"  Layers: {config.extraction.layers}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Load Episodes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load episodes from Phase 1\n",
    "episodes_collection = load_episodes(config.data.processed_dir / \"episodes.parquet\")\n",
    "episodes = episodes_collection.episodes\n",
    "\n",
    "print(f\"Loaded {len(episodes)} episodes\")\n",
    "print(f\"\\nCategory breakdown:\")\n",
    "summary = episodes_collection.summary()\n",
    "for cat, count in summary['categories'].items():\n",
    "    print(f\"  {cat}: {count}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Extract Activations\n",
    "\n",
    "Extract at 3 positions × 5 layers = 15 samples per episode.\n",
    "\n",
    "**WARNING:** This takes 1-2 hours on GPU depending on model size."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract activations\n",
    "dataset = extract_activations_batch(\n",
    "    episodes=episodes,\n",
    "    positions=config.extraction.positions,\n",
    "    layers=config.extraction.layers,\n",
    "    model_id=config.model.id,\n",
    "    save_path=config.data.processed_dir / \"activations.parquet\",\n",
    "    verbose=True,\n",
    ")\n",
    "\n",
    "print(f\"\\nExtracted {len(dataset)} activation samples\")\n",
    "print(f\"Activation shape: {dataset.activations.shape}\")\n",
    "print(f\"Hidden size: {dataset.hidden_size}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load activations (if already extracted)\n",
    "# dataset = load_activations(config.data.processed_dir / \"activations.parquet\")\n",
    "# print(f\"Loaded {len(dataset)} activation samples\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Summary\n",
    "summary = dataset.summary()\n",
    "print(\"\\nDataset summary:\")\n",
    "print(f\"  Samples: {summary['n_samples']}\")\n",
    "print(f\"  Positions: {summary['positions']}\")\n",
    "print(f\"  Layers: {summary['layers']}\")\n",
    "print(f\"  Categories: {summary['categories']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Train Probes\n",
    "\n",
    "Train two probes:\n",
    "- **Reality probe:** Predicts `tool_used` (ground truth)\n",
    "- **Narrative probe:** Predicts `claims_action` (model's claim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter to single position and layer for initial probe\n",
    "# Use mid_response at layer 16 (middle layer)\n",
    "probe_dataset = dataset.filter_by_position(\"mid_response\").filter_by_layer(16)\n",
    "\n",
    "print(f\"Probe training dataset: {len(probe_dataset)} samples\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train reality probe\n",
    "reality_probe, reality_train_metrics, reality_test_metrics = train_and_evaluate(\n",
    "    probe_dataset,\n",
    "    label_type=\"reality\",\n",
    "    test_size=0.2,\n",
    "    random_state=42,\n",
    ")\n",
    "\n",
    "print(\"\\nReality Probe (predicts tool_used):\")\n",
    "print(\"Train Metrics:\")\n",
    "print(reality_train_metrics)\n",
    "print(\"\\nTest Metrics:\")\n",
    "print(reality_test_metrics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train narrative probe\n",
    "narrative_probe, narrative_train_metrics, narrative_test_metrics = train_and_evaluate(\n",
    "    probe_dataset,\n",
    "    label_type=\"narrative\",\n",
    "    test_size=0.2,\n",
    "    random_state=42,\n",
    ")\n",
    "\n",
    "print(\"\\nNarrative Probe (predicts claims_action):\")\n",
    "print(\"Train Metrics:\")\n",
    "print(narrative_train_metrics)\n",
    "print(\"\\nTest Metrics:\")\n",
    "print(narrative_test_metrics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save probes\n",
    "save_probe(reality_probe, config.data.processed_dir / \"reality_probe.pkl\")\n",
    "save_probe(narrative_probe, config.data.processed_dir / \"narrative_probe.pkl\")\n",
    "\n",
    "print(\"Probes saved.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. CRITICAL: Position Analysis\n",
    "\n",
    "**Key test:** Does the probe work at `first_assistant` (before any tool tokens)?\n",
    "\n",
    "If yes → probe detects action-grounding, not syntax  \n",
    "If no → probe might just be detecting `<<CALL` tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train probes at each position (using layer 16)\n",
    "position_accuracies = {}\n",
    "\n",
    "for position in config.extraction.positions:\n",
    "    print(f\"\\nTraining probe at position: {position}\")\n",
    "    \n",
    "    pos_dataset = dataset.filter_by_position(position).filter_by_layer(16)\n",
    "    \n",
    "    if len(pos_dataset) == 0:\n",
    "        print(f\"  No samples found for {position}\")\n",
    "        continue\n",
    "    \n",
    "    probe, _, test_metrics = train_and_evaluate(\n",
    "        pos_dataset,\n",
    "        label_type=\"reality\",\n",
    "        random_state=42,\n",
    "    )\n",
    "    \n",
    "    position_accuracies[position] = test_metrics.accuracy\n",
    "    print(f\"  Test accuracy: {test_metrics.accuracy:.1%}\")\n",
    "\n",
    "print(\"\\nPosition accuracies:\")\n",
    "for pos, acc in position_accuracies.items():\n",
    "    status = \"✓ PASS\" if acc > 0.80 else \"✗ FAIL\"\n",
    "    print(f\"  {pos}: {acc:.1%} {status}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Investigating before_tool Underperformance\n",
    "\n",
    "**Problem:** The `before_tool` position shows lower accuracy (78.9%) than `first_assistant` (83.9%), which is counterintuitive.\n",
    "\n",
    "**Hypotheses:**\n",
    "- **A:** Position extraction is ill-defined for no-tool cases\n",
    "- **B:** Representation shifts from \"decision\" to \"execution\" \n",
    "- **C:** Token-level noise at before_tool position\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize position analysis\n",
    "fig = plot_position_accuracy(\n",
    "    position_accuracies,\n",
    "    title=\"Reality Probe Accuracy by Token Position\",\n",
    "    save_path=config.data.figures_dir / \"figure2_position_accuracy\",\n",
    ")\n",
    "\n",
    "plt.show()\n",
    "\n",
    "# Check critical result\n",
    "first_assistant_acc = position_accuracies.get('first_assistant', 0)\n",
    "if first_assistant_acc > 0.80:\n",
    "    print(f\"\\n✓ CRITICAL RESULT: first_assistant accuracy = {first_assistant_acc:.1%} > 80%\")\n",
    "    print(\"  → Probe detects action-grounding, not just syntax!\")\n",
    "else:\n",
    "    print(f\"\\n✗ WARNING: first_assistant accuracy = {first_assistant_acc:.1%} < 80%\")\n",
    "    print(\"  → May be detecting syntax, not action-grounding\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "uz"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 9.1 Position Extraction Diagnostics\n",
    "\n",
    "Check what positions are actually extracted for different cases, especially for `before_tool` when `tool_used=False`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check position extraction for different cases\n",
    "from collections import Counter\n",
    "\n",
    "# Count samples by position and tool_used\n",
    "position_tool_counts = {}\n",
    "for sample in dataset.samples:\n",
    "    key = (sample.position, sample.tool_used)\n",
    "    position_tool_counts[key] = position_tool_counts.get(key, 0) + 1\n",
    "\n",
    "print(\"Sample counts by position and tool_used:\")\n",
    "for (pos, tool_used), count in sorted(position_tool_counts.items()):\n",
    "    tool_used_str = str(tool_used)\n",
    "    print(f\"  {pos:20s} | tool_used={tool_used_str:5s} | n={count:4d}\")\n",
    "\n",
    "# Check specifically for before_tool\n",
    "before_tool_samples = [s for s in dataset.samples if s.position == \"before_tool\"]\n",
    "print(f\"\\nbefore_tool samples: {len(before_tool_samples)}\")\n",
    "print(f\"  tool_used=True:  {sum(1 for s in before_tool_samples if s.tool_used)}\")\n",
    "print(f\"  tool_used=False: {sum(1 for s in before_tool_samples if not s.tool_used)}\")\n",
    "\n",
    "# Check if before_tool samples have valid token_str\n",
    "valid_before_tool = [s for s in before_tool_samples if s.token_str is not None]\n",
    "print(f\"\\nbefore_tool samples with token_str: {len(valid_before_tool)}/{len(before_tool_samples)}\")\n",
    "\n",
    "# Sample some token strings\n",
    "if valid_before_tool:\n",
    "    print(\"\\nSample tokens at before_tool position:\")\n",
    "    for i, sample in enumerate(valid_before_tool[:10]):\n",
    "        token_repr = repr(sample.token_str) if sample.token_str else \"None\"\n",
    "        print(f\"  {i+1}. tool_used={sample.tool_used}, token={token_repr[:50]}\")\n",
    "    \n",
    "    # Print actual episodes for problematic cases (tool_used=False but token='<<')\n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\"Investigating problematic cases: tool_used=False but token='<<'\")\n",
    "    print(\"=\"*60)\n",
    "    \n",
    "    problematic_samples = [s for s in valid_before_tool if not s.tool_used and s.token_str and '<<' in s.token_str]\n",
    "    print(f\"\\nFound {len(problematic_samples)} samples with tool_used=False but token contains '<<'\")\n",
    "    \n",
    "    if problematic_samples:\n",
    "        print(\"\\nExample episodes (showing first 3):\")\n",
    "        for i, sample in enumerate(problematic_samples[:3]):\n",
    "            # Find the corresponding episode\n",
    "            episode = next((ep for ep in episodes if ep.id == sample.episode_id), None)\n",
    "            \n",
    "            if episode:\n",
    "                print(f\"\\n{'='*60}\")\n",
    "                print(f\"Example {i+1}: Episode ID = {sample.episode_id}\")\n",
    "                print(f\"  Position: {sample.position}, Layer: {sample.layer}\")\n",
    "                print(f\"  Token at before_tool: {repr(sample.token_str)}\")\n",
    "                print(f\"  tool_used: {sample.tool_used}\")\n",
    "                print(f\"  claims_action: {sample.claims_action}\")\n",
    "                print(f\"  Category: {sample.category}\")\n",
    "                print(f\"\\n  Episode details:\")\n",
    "                print(f\"    System variant: {episode.system_variant}\")\n",
    "                print(f\"    Social pressure: {episode.social_pressure}\")\n",
    "                print(f\"    Tool type: {episode.tool_type}\")\n",
    "                \n",
    "                # Show assistant reply (truncated)\n",
    "                if hasattr(episode, 'assistant_reply'):\n",
    "                    reply_preview = episode.assistant_reply[:500] if len(episode.assistant_reply) > 500 else episode.assistant_reply\n",
    "                    print(f\"\\n  Assistant reply (first 500 chars):\")\n",
    "                    print(f\"    {reply_preview}\")\n",
    "                    if len(episode.assistant_reply) > 500:\n",
    "                        print(f\"    ... (truncated, total length: {len(episode.assistant_reply)})\")\n",
    "                \n",
    "                # Check if there's tool call syntax in the reply\n",
    "                if hasattr(episode, 'assistant_reply'):\n",
    "                    has_call = '<<CALL' in episode.assistant_reply\n",
    "                    has_end = '>>' in episode.assistant_reply\n",
    "                    print(f\"\\n  Tool call syntax check:\")\n",
    "                    print(f\"    Contains '<<CALL': {has_call}\")\n",
    "                    print(f\"    Contains '>>': {has_end}\")\n",
    "                    \n",
    "                    # Extract tool call portion if present\n",
    "                    if '<<CALL' in episode.assistant_reply:\n",
    "                        call_start = episode.assistant_reply.find('<<CALL')\n",
    "                        call_end = episode.assistant_reply.find('>>', call_start)\n",
    "                        if call_end > call_start:\n",
    "                            call_text = episode.assistant_reply[call_start:call_end+2]\n",
    "                            print(f\"    Tool call text: {call_text[:200]}\")\n",
    "                        else:\n",
    "                            print(f\"    Tool call text (incomplete?): {episode.assistant_reply[call_start:call_start+200]}\")\n",
    "                \n",
    "                # Check tool_call_raw field (if available)\n",
    "                if hasattr(episode, 'tool_call_raw') and episode.tool_call_raw:\n",
    "                    print(f\"\\n  tool_call_raw field:\")\n",
    "                    print(f\"    {episode.tool_call_raw[:300]}\")\n",
    "                elif hasattr(episode, 'tool_call_raw'):\n",
    "                    print(f\"\\n  tool_call_raw: None (tool was not executed)\")\n",
    "                \n",
    "                # Show tool_call_args if available\n",
    "                if hasattr(episode, 'tool_call_args') and episode.tool_call_args:\n",
    "                    print(f\"\\n  tool_call_args:\")\n",
    "                    print(f\"    {episode.tool_call_args}\")\n",
    "            else:\n",
    "                print(f\"\\n  Episode {sample.episode_id} not found in episodes list\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split accuracy by tool_used at each position\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "split_accuracies = {}\n",
    "\n",
    "for position in config.extraction.positions:\n",
    "    pos_dataset = dataset.filter_by_position(position).filter_by_layer(16)\n",
    "    \n",
    "    if len(pos_dataset) == 0:\n",
    "        continue\n",
    "    \n",
    "    # Split by tool_used\n",
    "    tool_used_mask = np.array([s.tool_used for s in pos_dataset.samples])\n",
    "    tool_used_dataset = pos_dataset._filter_by_indices(np.where(tool_used_mask)[0])\n",
    "    no_tool_dataset = pos_dataset._filter_by_indices(np.where(~tool_used_mask)[0])\n",
    "    \n",
    "    print(f\"\\n{position}:\")\n",
    "    print(f\"  Total samples: {len(pos_dataset)}\")\n",
    "    print(f\"  tool_used=True: {len(tool_used_dataset)}\")\n",
    "    print(f\"  tool_used=False: {len(no_tool_dataset)}\")\n",
    "    \n",
    "    # Train probe on full dataset\n",
    "    probe, _, _ = train_and_evaluate(\n",
    "        pos_dataset,\n",
    "        label_type=\"reality\",\n",
    "        random_state=42,\n",
    "    )\n",
    "    \n",
    "    # Evaluate on each subset\n",
    "    if len(tool_used_dataset) > 0:\n",
    "        X_tool, y_tool = tool_used_dataset.to_sklearn_format(\"reality\")\n",
    "        y_pred_tool = probe.predict(X_tool)\n",
    "        acc_tool = accuracy_score(y_tool, y_pred_tool)\n",
    "        print(f\"  Accuracy on tool_used=True:  {acc_tool:.1%}\")\n",
    "        split_accuracies[(position, True)] = acc_tool\n",
    "    \n",
    "    if len(no_tool_dataset) > 0:\n",
    "        X_no_tool, y_no_tool = no_tool_dataset.to_sklearn_format(\"reality\")\n",
    "        y_pred_no_tool = probe.predict(X_no_tool)\n",
    "        acc_no_tool = accuracy_score(y_no_tool, y_pred_no_tool)\n",
    "        print(f\"  Accuracy on tool_used=False: {acc_no_tool:.1%}\")\n",
    "        split_accuracies[(position, False)] = acc_no_tool\n",
    "\n",
    "# Summary table\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"Summary: Accuracy by Position and tool_used\")\n",
    "print(\"=\"*60)\n",
    "for position in config.extraction.positions:\n",
    "    acc_true = split_accuracies.get((position, True), None)\n",
    "    acc_false = split_accuracies.get((position, False), None)\n",
    "    acc_overall = position_accuracies.get(position, None)\n",
    "    \n",
    "    print(f\"\\n{position}:\")\n",
    "    if acc_true is not None:\n",
    "        print(f\"  tool_used=True:  {acc_true:.1%}\")\n",
    "    if acc_false is not None:\n",
    "        print(f\"  tool_used=False: {acc_false:.1%}\")\n",
    "    if acc_overall is not None:\n",
    "        print(f\"  Overall:         {acc_overall:.1%}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 9.3 Probe Direction Comparison Across Positions\n",
    "\n",
    "Train separate probes at each position and compare their directions using cosine similarity.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train probes at each position and compare directions\n",
    "from src.analysis.probes import get_probe_direction, compare_probes\n",
    "from itertools import combinations\n",
    "\n",
    "position_probes = {}\n",
    "\n",
    "print(\"Training probes at each position...\")\n",
    "for position in config.extraction.positions:\n",
    "    pos_dataset = dataset.filter_by_position(position).filter_by_layer(16)\n",
    "    \n",
    "    if len(pos_dataset) == 0:\n",
    "        continue\n",
    "    \n",
    "    probe, _, _ = train_and_evaluate(\n",
    "        pos_dataset,\n",
    "        label_type=\"reality\",\n",
    "        random_state=42,\n",
    "    )\n",
    "    \n",
    "    position_probes[position] = probe\n",
    "    print(f\"  {position}: trained\")\n",
    "\n",
    "# Compare probe directions\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"Probe Direction Similarity Across Positions\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "for pos1, pos2 in combinations(position_probes.keys(), 2):\n",
    "    comparison = compare_probes(position_probes[pos1], position_probes[pos2], normalize=True)\n",
    "    cosine_sim = comparison['cosine_similarity']\n",
    "    \n",
    "    print(f\"\\n{pos1:20s} vs {pos2:20s}:\")\n",
    "    print(f\"  Cosine similarity: {cosine_sim:.3f}\")\n",
    "    print(f\"  L2 distance:       {comparison['l2_distance']:.3f}\")\n",
    "    \n",
    "    if abs(cosine_sim) > 0.9:\n",
    "        print(\"  → Very similar directions\")\n",
    "    elif abs(cosine_sim) > 0.7:\n",
    "        print(\"  → Similar directions\")\n",
    "    elif abs(cosine_sim) > 0.5:\n",
    "        print(\"  → Partially aligned\")\n",
    "    elif abs(cosine_sim) > 0.3:\n",
    "        print(\"  → Different directions\")\n",
    "    else:\n",
    "        print(\"  → Very different directions\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. 2D Probe Projection Analysis\n",
    "\n",
    "**Goal:** Visualize how reality and narrative probes diverge across categories to reveal the fake action mechanism.\n",
    "\n",
    "The 0.513 cosine similarity suggests partial alignment. We want to see:\n",
    "- Where fake_action cases fall (should be: low reality, high narrative)\n",
    "- Where true_action cases fall (should be: high reality, high narrative)\n",
    "- Where silent_action cases fall (should be: high reality, low narrative)\n",
    "- Where honest_no_action cases fall (should be: low reality, low narrative)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 10.1 Project Samples onto Probe Directions\n",
    "\n",
    "Get test dataset and project activations onto both reality and narrative probe directions.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get test dataset (already split from training)\n",
    "# Use the same test split as used for probe training\n",
    "train_dataset, test_dataset = probe_dataset.train_test_split(test_size=0.2, random_state=42)\n",
    "\n",
    "# Get activations and labels\n",
    "X_test, y_test_reality = test_dataset.to_sklearn_format(\"reality\")\n",
    "_, y_test_narrative = test_dataset.to_sklearn_format(\"narrative\")\n",
    "\n",
    "# Project onto probe directions\n",
    "# These are logit scores (before sigmoid)\n",
    "reality_scores = X_test @ reality_probe.coef_.T\n",
    "narrative_scores = X_test @ narrative_probe.coef_.T\n",
    "\n",
    "# Convert to probabilities for easier interpretation\n",
    "reality_probs = reality_probe.predict_proba(X_test)[:, 1]\n",
    "narrative_probs = narrative_probe.predict_proba(X_test)[:, 1]\n",
    "\n",
    "# Get category labels\n",
    "categories = np.array([s.category for s in test_dataset.samples])\n",
    "\n",
    "print(f\"Test dataset: {len(test_dataset)} samples\")\n",
    "print(f\"\\nCategory breakdown:\")\n",
    "for cat in ['true_action', 'fake_action', 'silent_action', 'honest_no_action', 'wrong_tool']:\n",
    "    count = np.sum(categories == cat)\n",
    "    print(f\"  {cat}: {count}\")\n",
    "\n",
    "print(f\"\\nReality scores range: [{reality_scores.min():.2f}, {reality_scores.max():.2f}]\")\n",
    "print(f\"Narrative scores range: [{narrative_scores.min():.2f}, {narrative_scores.max():.2f}]\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 10.2 2D Scatter Plot by Category\n",
    "\n",
    "Create scatter plot showing where each category falls in probe space.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create 2D scatter plot colored by category\n",
    "fig, ax = plt.subplots(figsize=(12, 10))\n",
    "\n",
    "# Color scheme for categories\n",
    "category_colors = {\n",
    "    'true_action': 'green',\n",
    "    'fake_action': 'red',\n",
    "    'silent_action': 'orange',\n",
    "    'honest_no_action': 'blue',\n",
    "    'wrong_tool': 'purple',\n",
    "}\n",
    "\n",
    "category_labels = {\n",
    "    'true_action': 'True Action',\n",
    "    'fake_action': 'Fake Action',\n",
    "    'silent_action': 'Silent Action',\n",
    "    'honest_no_action': 'Honest No Action',\n",
    "    'wrong_tool': 'Wrong Tool',\n",
    "}\n",
    "\n",
    "# Plot each category\n",
    "for cat in ['true_action', 'fake_action', 'silent_action', 'honest_no_action', 'wrong_tool']:\n",
    "    mask = categories == cat\n",
    "    if np.any(mask):\n",
    "        ax.scatter(\n",
    "            reality_scores[mask],\n",
    "            narrative_scores[mask],\n",
    "            label=category_labels[cat],\n",
    "            alpha=0.6,\n",
    "            s=50,\n",
    "            color=category_colors[cat],\n",
    "            edgecolors='black',\n",
    "            linewidths=0.5,\n",
    "        )\n",
    "\n",
    "# Add reference lines at 0\n",
    "ax.axhline(0, c='k', ls='--', alpha=0.3, linewidth=1)\n",
    "ax.axvline(0, c='k', ls='--', alpha=0.3, linewidth=1)\n",
    "\n",
    "# Add decision boundaries (logit threshold = 0 corresponds to prob = 0.5)\n",
    "# For logistic regression, decision boundary is at logit = 0\n",
    "ax.axhline(0, c='gray', ls=':', alpha=0.5, linewidth=1, label='Decision boundary')\n",
    "ax.axvline(0, c='gray', ls=':', alpha=0.5, linewidth=1)\n",
    "\n",
    "# Labels and title\n",
    "ax.set_xlabel('Reality Probe Score (logit)', fontsize=14)\n",
    "ax.set_ylabel('Narrative Probe Score (logit)', fontsize=14)\n",
    "ax.set_title('2D Probe Projection: Reality vs Narrative', fontsize=16, fontweight='bold')\n",
    "ax.legend(loc='best', fontsize=12)\n",
    "ax.grid(True, alpha=0.3)\n",
    "\n",
    "# Add quadrant labels\n",
    "ax.text(0.02, 0.98, 'Upper Right:\\nHigh Reality\\nHigh Narrative', \n",
    "        transform=ax.transAxes, fontsize=10, verticalalignment='top',\n",
    "        bbox=dict(boxstyle='round', facecolor='wheat', alpha=0.5))\n",
    "ax.text(0.02, 0.02, 'Lower Left:\\nLow Reality\\nLow Narrative', \n",
    "        transform=ax.transAxes, fontsize=10, verticalalignment='bottom',\n",
    "        bbox=dict(boxstyle='round', facecolor='wheat', alpha=0.5))\n",
    "ax.text(0.98, 0.98, 'Upper Left:\\nLow Reality\\nHigh Narrative\\n(FAKE ACTION)', \n",
    "        transform=ax.transAxes, fontsize=10, verticalalignment='top', horizontalalignment='right',\n",
    "        bbox=dict(boxstyle='round', facecolor='lightcoral', alpha=0.5))\n",
    "ax.text(0.98, 0.02, 'Lower Right:\\nHigh Reality\\nLow Narrative\\n(SILENT ACTION)', \n",
    "        transform=ax.transAxes, fontsize=10, verticalalignment='bottom', horizontalalignment='right',\n",
    "        bbox=dict(boxstyle='round', facecolor='lightblue', alpha=0.5))\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(config.data.figures_dir / \"figure_probe_2d_projection.png\", dpi=300, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(\"Expected pattern:\")\n",
    "print(\"  - true_action: upper right (high reality, high narrative)\")\n",
    "print(\"  - fake_action: upper left (low reality, high narrative)\")\n",
    "print(\"  - silent_action: lower right (high reality, low narrative)\")\n",
    "print(\"  - honest_no_action: lower left (low reality, low narrative)\")\n",
    "print(\"  - wrong_tool: high reality, variable narrative\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 10.3 Disagreement Analysis\n",
    "\n",
    "Find cases where reality and narrative probes disagree and analyze by category.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get predictions from both probes\n",
    "reality_pred = reality_probe.predict(X_test)\n",
    "narrative_pred = narrative_probe.predict(X_test)\n",
    "\n",
    "# Find disagreements\n",
    "disagreements = reality_pred != narrative_pred\n",
    "disagreement_rate = np.mean(disagreements)\n",
    "\n",
    "print(\"=\"*60)\n",
    "print(\"Probe Disagreement Analysis\")\n",
    "print(\"=\"*60)\n",
    "print(f\"\\nOverall disagreement rate: {disagreement_rate:.1%}\")\n",
    "print(f\"  Agree: {np.sum(~disagreements)} samples\")\n",
    "print(f\"  Disagree: {np.sum(disagreements)} samples\")\n",
    "\n",
    "# Analyze disagreements by category\n",
    "print(\"\\nDisagreements by category:\")\n",
    "disagreement_by_cat = {}\n",
    "for cat in ['true_action', 'fake_action', 'silent_action', 'honest_no_action', 'wrong_tool']:\n",
    "    cat_mask = categories == cat\n",
    "    if np.any(cat_mask):\n",
    "        cat_disagreements = disagreements[cat_mask]\n",
    "        cat_disagreement_rate = np.mean(cat_disagreements)\n",
    "        cat_total = np.sum(cat_mask)\n",
    "        cat_disagree_count = np.sum(cat_disagreements)\n",
    "        disagreement_by_cat[cat] = {\n",
    "            'rate': cat_disagreement_rate,\n",
    "            'count': cat_disagree_count,\n",
    "            'total': cat_total,\n",
    "        }\n",
    "        print(f\"\\n  {cat}:\")\n",
    "        print(f\"    Disagreement rate: {cat_disagreement_rate:.1%} ({cat_disagree_count}/{cat_total})\")\n",
    "\n",
    "# Analyze disagreement types\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"Disagreement Types\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Reality says 0 (no tool), Narrative says 1 (claims action)\n",
    "reality_0_narr_1 = (reality_pred == 0) & (narrative_pred == 1)\n",
    "# Reality says 1 (tool used), Narrative says 0 (no claim)\n",
    "reality_1_narr_0 = (reality_pred == 1) & (narrative_pred == 0)\n",
    "\n",
    "print(f\"\\nReality=0, Narrative=1 (reality says no tool, narrative claims action):\")\n",
    "print(f\"  Total: {np.sum(reality_0_narr_1)}\")\n",
    "for cat in ['true_action', 'fake_action', 'silent_action', 'honest_no_action', 'wrong_tool']:\n",
    "    cat_mask = categories == cat\n",
    "    count = np.sum(reality_0_narr_1 & cat_mask)\n",
    "    if count > 0:\n",
    "        print(f\"    {cat}: {count}\")\n",
    "\n",
    "print(f\"\\nReality=1, Narrative=0 (reality says tool used, narrative doesn't claim):\")\n",
    "print(f\"  Total: {np.sum(reality_1_narr_0)}\")\n",
    "for cat in ['true_action', 'fake_action', 'silent_action', 'honest_no_action', 'wrong_tool']:\n",
    "    cat_mask = categories == cat\n",
    "    count = np.sum(reality_1_narr_0 & cat_mask)\n",
    "    if count > 0:\n",
    "        print(f\"    {cat}: {count}\")\n",
    "\n",
    "# Create confusion matrix of probe disagreements\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"Probe Disagreement Confusion Matrix\")\n",
    "print(\"=\"*60)\n",
    "print(\"\\nRows = Reality Probe, Columns = Narrative Probe\")\n",
    "print(\"         | Claims=0  | Claims=1\")\n",
    "print(\"---------|----------|----------\")\n",
    "print(f\"No Tool  | {np.sum((reality_pred==0) & (narrative_pred==0)):8d} | {np.sum((reality_pred==0) & (narrative_pred==1)):8d}\")\n",
    "print(f\"Tool     | {np.sum((reality_pred==1) & (narrative_pred==0)):8d} | {np.sum((reality_pred==1) & (narrative_pred==1)):8d}\")\n",
    "\n",
    "# Cross-tabulation by category\n",
    "print(\"\\nDisagreement breakdown by category:\")\n",
    "disagreement_crosstab = pd.crosstab(\n",
    "    categories[disagreements],\n",
    "    pd.Series([\n",
    "        'Reality=0,Narr=1' if r == 0 else 'Reality=1,Narr=0'\n",
    "        for r, n in zip(reality_pred[disagreements], narrative_pred[disagreements])\n",
    "    ]),\n",
    "    margins=True\n",
    ")\n",
    "print(disagreement_crosstab)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 10.4 Quantitative Summary\n",
    "\n",
    "Compute mean scores, separation metrics, and quadrant distribution for each category.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Quantitative analysis by category\n",
    "print(\"=\"*60)\n",
    "print(\"Quantitative Summary by Category\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "category_stats = {}\n",
    "\n",
    "for cat in ['true_action', 'fake_action', 'silent_action', 'honest_no_action', 'wrong_tool']:\n",
    "    mask = categories == cat\n",
    "    if not np.any(mask):\n",
    "        continue\n",
    "    \n",
    "    cat_reality_scores = reality_scores[mask]\n",
    "    cat_narrative_scores = narrative_scores[mask]\n",
    "    cat_reality_probs = reality_probs[mask]\n",
    "    cat_narrative_probs = narrative_probs[mask]\n",
    "    \n",
    "    stats = {\n",
    "        'n': np.sum(mask),\n",
    "        'reality_mean_score': np.mean(cat_reality_scores),\n",
    "        'reality_std_score': np.std(cat_reality_scores),\n",
    "        'reality_mean_prob': np.mean(cat_reality_probs),\n",
    "        'narrative_mean_score': np.mean(cat_narrative_scores),\n",
    "        'narrative_std_score': np.std(cat_narrative_scores),\n",
    "        'narrative_mean_prob': np.mean(cat_narrative_probs),\n",
    "    }\n",
    "    \n",
    "    category_stats[cat] = stats\n",
    "    \n",
    "    print(f\"\\n{cat}:\")\n",
    "    print(f\"  N samples: {stats['n']}\")\n",
    "    print(f\"  Reality probe:\")\n",
    "    print(f\"    Mean score (logit): {stats['reality_mean_score']:.3f} ± {stats['reality_std_score']:.3f}\")\n",
    "    print(f\"    Mean probability:   {stats['reality_mean_prob']:.3f}\")\n",
    "    print(f\"  Narrative probe:\")\n",
    "    print(f\"    Mean score (logit): {stats['narrative_mean_score']:.3f} ± {stats['narrative_std_score']:.3f}\")\n",
    "    print(f\"    Mean probability:   {stats['narrative_mean_prob']:.3f}\")\n",
    "\n",
    "# Compute separation metrics\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"Separation Metrics\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Distance between fake_action and true_action clusters\n",
    "if 'fake_action' in category_stats and 'true_action' in category_stats:\n",
    "    fake_center = np.array([\n",
    "        category_stats['fake_action']['reality_mean_score'],\n",
    "        category_stats['fake_action']['narrative_mean_score']\n",
    "    ])\n",
    "    true_center = np.array([\n",
    "        category_stats['true_action']['reality_mean_score'],\n",
    "        category_stats['true_action']['narrative_mean_score']\n",
    "    ])\n",
    "    distance = np.linalg.norm(fake_center - true_center)\n",
    "    print(f\"\\nDistance between fake_action and true_action centers: {distance:.3f}\")\n",
    "\n",
    "# Quadrant distribution\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"Quadrant Distribution\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Define quadrants based on logit scores (0 is decision boundary)\n",
    "quadrants = {\n",
    "    'Upper Right (High Reality, High Narrative)': (reality_scores > 0) & (narrative_scores > 0),\n",
    "    'Upper Left (Low Reality, High Narrative)': (reality_scores <= 0) & (narrative_scores > 0),\n",
    "    'Lower Left (Low Reality, Low Narrative)': (reality_scores <= 0) & (narrative_scores <= 0),\n",
    "    'Lower Right (High Reality, Low Narrative)': (reality_scores > 0) & (narrative_scores <= 0),\n",
    "}\n",
    "\n",
    "print(\"\\nOverall quadrant distribution:\")\n",
    "for quadrant_name, quadrant_mask in quadrants.items():\n",
    "    count = np.sum(quadrant_mask)\n",
    "    pct = count / len(categories) * 100\n",
    "    print(f\"  {quadrant_name}: {count:4d} ({pct:5.1f}%)\")\n",
    "\n",
    "print(\"\\nQuadrant distribution by category:\")\n",
    "for cat in ['true_action', 'fake_action', 'silent_action', 'honest_no_action', 'wrong_tool']:\n",
    "    cat_mask = categories == cat\n",
    "    if not np.any(cat_mask):\n",
    "        continue\n",
    "    \n",
    "    print(f\"\\n  {cat}:\")\n",
    "    for quadrant_name, quadrant_mask in quadrants.items():\n",
    "        count = np.sum(quadrant_mask & cat_mask)\n",
    "        pct = count / np.sum(cat_mask) * 100 if np.sum(cat_mask) > 0 else 0\n",
    "        print(f\"    {quadrant_name}: {count:4d} ({pct:5.1f}%)\")\n",
    "\n",
    "# Expected vs observed patterns\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"Expected vs Observed Patterns\")\n",
    "print(\"=\"*60)\n",
    "print(\"\\nExpected:\")\n",
    "print(\"  fake_action → Upper Left (Low Reality, High Narrative)\")\n",
    "print(\"  true_action → Upper Right (High Reality, High Narrative)\")\n",
    "print(\"  silent_action → Lower Right (High Reality, Low Narrative)\")\n",
    "print(\"  honest_no_action → Lower Left (Low Reality, Low Narrative)\")\n",
    "\n",
    "# Check if fake_action is primarily in upper left\n",
    "if 'fake_action' in category_stats:\n",
    "    fake_mask = categories == 'fake_action'\n",
    "    fake_upper_left = np.sum((reality_scores[fake_mask] <= 0) & (narrative_scores[fake_mask] > 0))\n",
    "    fake_total = np.sum(fake_mask)\n",
    "    fake_upper_left_pct = fake_upper_left / fake_total * 100 if fake_total > 0 else 0\n",
    "    print(f\"\\nObserved:\")\n",
    "    print(f\"  fake_action in Upper Left: {fake_upper_left}/{fake_total} ({fake_upper_left_pct:.1f}%)\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 9.4 Token-Level Analysis\n",
    "\n",
    "Visualize actual tokens at each position to check for token-level noise or variation.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze token distribution at each position\n",
    "from collections import Counter\n",
    "\n",
    "print(\"Token analysis at each position:\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "for position in config.extraction.positions:\n",
    "    pos_samples = [s for s in dataset.samples if s.position == position and s.layer == 16]\n",
    "    \n",
    "    if len(pos_samples) == 0:\n",
    "        continue\n",
    "    \n",
    "    # Get token strings\n",
    "    tokens = [s.token_str for s in pos_samples if s.token_str is not None]\n",
    "    \n",
    "    print(f\"\\n{position}:\")\n",
    "    print(f\"  Total samples: {len(pos_samples)}\")\n",
    "    print(f\"  Samples with token_str: {len(tokens)}\")\n",
    "    \n",
    "    if len(tokens) > 0:\n",
    "        # Count unique tokens\n",
    "        token_counts = Counter(tokens)\n",
    "        print(f\"  Unique tokens: {len(token_counts)}\")\n",
    "        \n",
    "        # Show most common tokens\n",
    "        print(f\"  Most common tokens:\")\n",
    "        for token, count in token_counts.most_common(10):\n",
    "            token_repr = repr(token) if token else \"None\"\n",
    "            pct = count / len(tokens) * 100\n",
    "            print(f\"    {token_repr[:40]:40s} ({count:3d}, {pct:5.1f}%)\")\n",
    "        \n",
    "        # Check for common patterns (newlines, spaces, punctuation)\n",
    "        newline_tokens = sum(1 for t in tokens if t and '\\n' in t)\n",
    "        space_tokens = sum(1 for t in tokens if t and t.strip() == '')\n",
    "        print(f\"\\n  Token patterns:\")\n",
    "        print(f\"    Contains newline: {newline_tokens:3d} ({newline_tokens/len(tokens)*100:5.1f}%)\")\n",
    "        print(f\"    Whitespace only:  {space_tokens:3d} ({space_tokens/len(tokens)*100:5.1f}%)\")\n",
    "\n",
    "# Compare before_tool tokens for tool_used=True vs False\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"before_tool Token Comparison: tool_used=True vs False\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "before_tool_samples = [s for s in dataset.samples if s.position == \"before_tool\" and s.layer == 16]\n",
    "tool_tokens = [s.token_str for s in before_tool_samples if s.tool_used and s.token_str]\n",
    "no_tool_tokens = [s.token_str for s in before_tool_samples if not s.tool_used and s.token_str]\n",
    "\n",
    "print(f\"\\ntool_used=True:  {len(tool_tokens)} samples\")\n",
    "if tool_tokens:\n",
    "    tool_counts = Counter(tool_tokens)\n",
    "    print(f\"  Unique tokens: {len(tool_counts)}\")\n",
    "    print(f\"  Top tokens:\")\n",
    "    for token, count in tool_counts.most_common(5):\n",
    "        print(f\"    {repr(token)[:40]:40s} ({count})\")\n",
    "\n",
    "print(f\"\\ntool_used=False: {len(no_tool_tokens)} samples\")\n",
    "if no_tool_tokens:\n",
    "    no_tool_counts = Counter(no_tool_tokens)\n",
    "    print(f\"  Unique tokens: {len(no_tool_counts)}\")\n",
    "    print(f\"  Top tokens:\")\n",
    "    for token, count in no_tool_counts.most_common(5):\n",
    "        print(f\"    {repr(token)[:40]:40s} ({count})\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Analyze Fake Action Cases\n",
    "\n",
    "**Critical test:** Does the probe correctly identify fake action episodes?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze probe on fake action cases\n",
    "fake_analysis = analyze_probe_on_category(\n",
    "    reality_probe,\n",
    "    probe_dataset,\n",
    "    category=\"fake_action\",\n",
    "    label_type=\"reality\",\n",
    ")\n",
    "\n",
    "print(f\"\\nFake Action Analysis:\")\n",
    "print(f\"  N samples: {fake_analysis['n_samples']}\")\n",
    "print(f\"  Metrics:\")\n",
    "print(fake_analysis['metrics'])\n",
    "\n",
    "# Check alignment with ground truth\n",
    "fake_probs = fake_analysis['probabilities']\n",
    "fake_preds = fake_analysis['predictions']\n",
    "fake_labels = fake_analysis['true_labels']  # Should all be 0 (tool not used)\n",
    "\n",
    "# How many does probe correctly identify as \"tool not used\"?\n",
    "correct_on_fakes = np.mean(fake_preds == 0)\n",
    "mean_prob_tool_used = np.mean(fake_probs)\n",
    "\n",
    "print(f\"\\n**Probe on Fake Actions:**\")\n",
    "print(f\"  Correctly predicts 'no tool': {correct_on_fakes:.1%}\")\n",
    "print(f\"  Mean P(tool_used): {mean_prob_tool_used:.3f}\")\n",
    "\n",
    "if correct_on_fakes > 0.95:\n",
    "    print(\"  ✓ Probe aligns with reality, not narrative!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Histogram of P(tool_used) on fake vs true cases\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# Get predictions on true action cases for comparison\n",
    "true_analysis = analyze_probe_on_category(\n",
    "    reality_probe,\n",
    "    probe_dataset,\n",
    "    category=\"true_action\",\n",
    "    label_type=\"reality\",\n",
    ")\n",
    "\n",
    "# Plot fake action probabilities\n",
    "axes[0].hist(fake_probs, bins=20, alpha=0.7, color='red', edgecolor='black')\n",
    "axes[0].axvline(x=0.5, color='k', linestyle='--', linewidth=1)\n",
    "axes[0].set_xlabel('P(tool_used)')\n",
    "axes[0].set_ylabel('Count')\n",
    "axes[0].set_title(f'Fake Actions (should cluster near 0)\\nMean = {mean_prob_tool_used:.3f}')\n",
    "axes[0].set_xlim(0, 1)\n",
    "\n",
    "# Plot true action probabilities\n",
    "true_probs = true_analysis['probabilities']\n",
    "mean_prob_true = np.mean(true_probs)\n",
    "axes[1].hist(true_probs, bins=20, alpha=0.7, color='green', edgecolor='black')\n",
    "axes[1].axvline(x=0.5, color='k', linestyle='--', linewidth=1)\n",
    "axes[1].set_xlabel('P(tool_used)')\n",
    "axes[1].set_ylabel('Count')\n",
    "axes[1].set_title(f'True Actions (should cluster near 1)\\nMean = {mean_prob_true:.3f}')\n",
    "axes[1].set_xlim(0, 1)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(config.data.figures_dir / \"figure3_fake_vs_true_probs.png\", dpi=300, bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Layer Analysis\n",
    "\n",
    "Which layers encode action-grounding information?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train probes at each layer (using mid_response position)\n",
    "layer_accuracies = {}\n",
    "\n",
    "for layer in config.extraction.layers:\n",
    "    print(f\"\\nTraining probe at layer: {layer}\")\n",
    "    \n",
    "    layer_dataset = dataset.filter_by_position(\"mid_response\").filter_by_layer(layer)\n",
    "    \n",
    "    if len(layer_dataset) == 0:\n",
    "        print(f\"  No samples found for layer {layer}\")\n",
    "        continue\n",
    "    \n",
    "    probe, _, test_metrics = train_and_evaluate(\n",
    "        layer_dataset,\n",
    "        label_type=\"reality\",\n",
    "        random_state=42,\n",
    "    )\n",
    "    \n",
    "    layer_accuracies[layer] = test_metrics.accuracy\n",
    "    print(f\"  Test accuracy: {test_metrics.accuracy:.1%}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize layer analysis\n",
    "fig = plot_layer_analysis(\n",
    "    layer_accuracies,\n",
    "    title=\"Reality Probe Accuracy by Layer\",\n",
    "    save_path=config.data.figures_dir / \"figure5_layer_accuracy\",\n",
    ")\n",
    "\n",
    "plt.show()\n",
    "\n",
    "# Best layer\n",
    "best_layer = max(layer_accuracies.items(), key=lambda x: x[1])\n",
    "print(f\"\\nBest layer: {best_layer[0]} (accuracy: {best_layer[1]:.1%})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Probe Direction Analysis\n",
    "\n",
    "Are reality and narrative probes learning the same representation?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare probe directions\n",
    "from src.analysis.probes import compare_probes\n",
    "\n",
    "comparison = compare_probes(reality_probe, narrative_probe, normalize=True)\n",
    "\n",
    "print(f\"\\nReality vs Narrative Probe:\")\n",
    "print(f\"  Cosine similarity: {comparison['cosine_similarity']:.3f}\")\n",
    "print(f\"  L2 distance: {comparison['l2_distance']:.3f}\")\n",
    "\n",
    "if abs(comparison['cosine_similarity']) > 0.8:\n",
    "    print(\"  → Probes learn similar directions (aligned)\")\n",
    "elif abs(comparison['cosine_similarity']) < 0.3:\n",
    "    print(\"  → Probes learn different directions (independent representations)\")\n",
    "else:\n",
    "    print(\"  → Probes partially aligned\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Visualizations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get test data for reality probe\n",
    "train_dataset, test_dataset = probe_dataset.train_test_split(test_size=0.2, random_state=42)\n",
    "X_test, y_test = test_dataset.to_sklearn_format(\"reality\")\n",
    "\n",
    "# Confusion matrix\n",
    "y_pred = reality_probe.predict(X_test)\n",
    "fig = plot_confusion_matrix(\n",
    "    y_test,\n",
    "    y_pred,\n",
    "    labels=[\"No Tool\", \"Tool Used\"],\n",
    "    title=\"Reality Probe Confusion Matrix\",\n",
    "    save_path=config.data.figures_dir / \"reality_probe_confusion\",\n",
    ")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ROC curve\n",
    "y_proba = reality_probe.predict_proba(X_test)[:, 1]\n",
    "auc, fpr, tpr, thresholds = compute_roc_auc(y_test, y_proba)\n",
    "\n",
    "fig = plot_roc_curve(\n",
    "    fpr,\n",
    "    tpr,\n",
    "    auc,\n",
    "    title=\"Reality Probe ROC Curve\",\n",
    "    save_path=config.data.figures_dir / \"reality_probe_roc\",\n",
    ")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\" * 60)\n",
    "print(\"PHASE 2 RESULTS: MECHANISTIC PROBES\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "print(f\"\\nReality Probe Performance:\")\n",
    "print(f\"  Test Accuracy: {reality_test_metrics.accuracy:.1%}\")\n",
    "print(f\"  ROC-AUC: {reality_test_metrics.roc_auc:.3f}\")\n",
    "\n",
    "print(f\"\\nPosition Analysis:\")\n",
    "for pos, acc in position_accuracies.items():\n",
    "    print(f\"  {pos}: {acc:.1%}\")\n",
    "\n",
    "print(f\"\\nFake Action Analysis:\")\n",
    "print(f\"  Correct on fakes: {correct_on_fakes:.1%}\")\n",
    "print(f\"  Mean P(tool_used) on fakes: {mean_prob_tool_used:.3f}\")\n",
    "\n",
    "print(f\"\\nProbe Direction Comparison:\")\n",
    "print(f\"  Cosine similarity: {comparison['cosine_similarity']:.3f}\")\n",
    "\n",
    "print(\"\\n✓ Phase 2 complete: Linear probe can detect ground truth\")\n",
    "print(\"=\" * 60)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Next Steps\n",
    "\n",
    "→ **Notebook 03:** Test cross-tool generalization"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
