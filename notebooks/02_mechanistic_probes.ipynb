{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Notebook 02: Mechanistic Probes\n",
    "\n",
    "**Research Question:** Can we detect ground truth action-taking from model activations?\n",
    "\n",
    "This notebook:\n",
    "1. Extracts activations at multiple positions and layers\n",
    "2. Trains reality (tool_used) and narrative (claims_action) probes\n",
    "3. **Critical:** Position analysis (first_assistant vs before_tool)\n",
    "4. Analyzes probe behavior on fake action cases\n",
    "\n",
    "**Key hypothesis:** If `first_assistant` accuracy > 80%, probe is detecting action-grounding, not just tool syntax."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('..')\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from pathlib import Path\n",
    "\n",
    "from src.utils.logging import setup_logging\n",
    "from src.config import get_config\n",
    "from src.data.io import load_episodes, save_activations, load_activations\n",
    "from src.extraction import extract_activations_batch\n",
    "from src.analysis.probes import train_and_evaluate, analyze_probe_on_category, save_probe\n",
    "from src.analysis.visualization import (\n",
    "    plot_confusion_matrix,\n",
    "    plot_roc_curve,\n",
    "    plot_position_accuracy,\n",
    "    plot_layer_analysis,\n",
    ")\n",
    "from src.analysis.statistics import compute_roc_auc, bootstrap_metrics\n",
    "\n",
    "setup_logging(level=\"INFO\")\n",
    "config = get_config()\n",
    "\n",
    "print(f\"Extraction config:\")\n",
    "print(f\"  Positions: {config.extraction.positions}\")\n",
    "print(f\"  Layers: {config.extraction.layers}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Load Episodes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load episodes from Phase 1\n",
    "episodes_collection = load_episodes(config.data.processed_dir / \"episodes.parquet\")\n",
    "episodes = episodes_collection.episodes\n",
    "\n",
    "print(f\"Loaded {len(episodes)} episodes\")\n",
    "print(f\"\\nCategory breakdown:\")\n",
    "summary = episodes_collection.summary()\n",
    "for cat, count in summary['categories'].items():\n",
    "    print(f\"  {cat}: {count}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Extract Activations\n",
    "\n",
    "Extract at 3 positions × 5 layers = 15 samples per episode.\n",
    "\n",
    "**WARNING:** This takes 1-2 hours on GPU depending on model size."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract activations\n",
    "dataset = extract_activations_batch(\n",
    "    episodes=episodes,\n",
    "    positions=config.extraction.positions,\n",
    "    layers=config.extraction.layers,\n",
    "    model_id=config.model.id,\n",
    "    save_path=config.data.processed_dir / \"activations.parquet\",\n",
    "    verbose=True,\n",
    ")\n",
    "\n",
    "print(f\"\\nExtracted {len(dataset)} activation samples\")\n",
    "print(f\"Activation shape: {dataset.activations.shape}\")\n",
    "print(f\"Hidden size: {dataset.hidden_size}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load activations (if already extracted)\n",
    "# dataset = load_activations(config.data.processed_dir / \"activations.parquet\")\n",
    "# print(f\"Loaded {len(dataset)} activation samples\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Summary\n",
    "summary = dataset.summary()\n",
    "print(\"\\nDataset summary:\")\n",
    "print(f\"  Samples: {summary['n_samples']}\")\n",
    "print(f\"  Positions: {summary['positions']}\")\n",
    "print(f\"  Layers: {summary['layers']}\")\n",
    "print(f\"  Categories: {summary['categories']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Train Probes\n",
    "\n",
    "Train two probes:\n",
    "- **Reality probe:** Predicts `tool_used` (ground truth)\n",
    "- **Narrative probe:** Predicts `claims_action` (model's claim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter to single position and layer for initial probe\n",
    "# Use mid_response at layer 16 (middle layer)\n",
    "probe_dataset = dataset.filter_by_position(\"mid_response\").filter_by_layer(16)\n",
    "\n",
    "print(f\"Probe training dataset: {len(probe_dataset)} samples\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train reality probe\n",
    "reality_probe, reality_train_metrics, reality_test_metrics = train_and_evaluate(\n",
    "    probe_dataset,\n",
    "    label_type=\"reality\",\n",
    "    test_size=0.2,\n",
    "    random_state=42,\n",
    ")\n",
    "\n",
    "print(\"\\nReality Probe (predicts tool_used):\")\n",
    "print(\"Train Metrics:\")\n",
    "print(reality_train_metrics)\n",
    "print(\"\\nTest Metrics:\")\n",
    "print(reality_test_metrics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train narrative probe\n",
    "narrative_probe, narrative_train_metrics, narrative_test_metrics = train_and_evaluate(\n",
    "    probe_dataset,\n",
    "    label_type=\"narrative\",\n",
    "    test_size=0.2,\n",
    "    random_state=42,\n",
    ")\n",
    "\n",
    "print(\"\\nNarrative Probe (predicts claims_action):\")\n",
    "print(\"Train Metrics:\")\n",
    "print(narrative_train_metrics)\n",
    "print(\"\\nTest Metrics:\")\n",
    "print(narrative_test_metrics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save probes\n",
    "save_probe(reality_probe, config.data.processed_dir / \"reality_probe.pkl\")\n",
    "save_probe(narrative_probe, config.data.processed_dir / \"narrative_probe.pkl\")\n",
    "\n",
    "print(\"Probes saved.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. CRITICAL: Position Analysis\n",
    "\n",
    "**Key test:** Does the probe work at `first_assistant` (before any tool tokens)?\n",
    "\n",
    "If yes → probe detects action-grounding, not syntax  \n",
    "If no → probe might just be detecting `<<CALL` tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train probes at each position (using layer 16)\n",
    "position_accuracies = {}\n",
    "\n",
    "for position in config.extraction.positions:\n",
    "    print(f\"\\nTraining probe at position: {position}\")\n",
    "    \n",
    "    pos_dataset = dataset.filter_by_position(position).filter_by_layer(16)\n",
    "    \n",
    "    if len(pos_dataset) == 0:\n",
    "        print(f\"  No samples found for {position}\")\n",
    "        continue\n",
    "    \n",
    "    probe, _, test_metrics = train_and_evaluate(\n",
    "        pos_dataset,\n",
    "        label_type=\"reality\",\n",
    "        random_state=42,\n",
    "    )\n",
    "    \n",
    "    position_accuracies[position] = test_metrics.accuracy\n",
    "    print(f\"  Test accuracy: {test_metrics.accuracy:.1%}\")\n",
    "\n",
    "print(\"\\nPosition accuracies:\")\n",
    "for pos, acc in position_accuracies.items():\n",
    "    status = \"✓ PASS\" if acc > 0.80 else \"✗ FAIL\"\n",
    "    print(f\"  {pos}: {acc:.1%} {status}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize position analysis\n",
    "fig = plot_position_accuracy(\n",
    "    position_accuracies,\n",
    "    title=\"Reality Probe Accuracy by Token Position\",\n",
    "    save_path=config.data.figures_dir / \"figure2_position_accuracy\",\n",
    ")\n",
    "\n",
    "plt.show()\n",
    "\n",
    "# Check critical result\n",
    "first_assistant_acc = position_accuracies.get('first_assistant', 0)\n",
    "if first_assistant_acc > 0.80:\n",
    "    print(f\"\\n✓ CRITICAL RESULT: first_assistant accuracy = {first_assistant_acc:.1%} > 80%\")\n",
    "    print(\"  → Probe detects action-grounding, not just syntax!\")\n",
    "else:\n",
    "    print(f\"\\n✗ WARNING: first_assistant accuracy = {first_assistant_acc:.1%} < 80%\")\n",
    "    print(\"  → May be detecting syntax, not action-grounding\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Analyze Fake Action Cases\n",
    "\n",
    "**Critical test:** Does the probe correctly identify fake action episodes?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze probe on fake action cases\n",
    "fake_analysis = analyze_probe_on_category(\n",
    "    reality_probe,\n",
    "    probe_dataset,\n",
    "    category=\"fake_action\",\n",
    "    label_type=\"reality\",\n",
    ")\n",
    "\n",
    "print(f\"\\nFake Action Analysis:\")\n",
    "print(f\"  N samples: {fake_analysis['n_samples']}\")\n",
    "print(f\"  Metrics:\")\n",
    "print(fake_analysis['metrics'])\n",
    "\n",
    "# Check alignment with ground truth\n",
    "fake_probs = fake_analysis['probabilities']\n",
    "fake_preds = fake_analysis['predictions']\n",
    "fake_labels = fake_analysis['true_labels']  # Should all be 0 (tool not used)\n",
    "\n",
    "# How many does probe correctly identify as \"tool not used\"?\n",
    "correct_on_fakes = np.mean(fake_preds == 0)\n",
    "mean_prob_tool_used = np.mean(fake_probs)\n",
    "\n",
    "print(f\"\\n**Probe on Fake Actions:**\")\n",
    "print(f\"  Correctly predicts 'no tool': {correct_on_fakes:.1%}\")\n",
    "print(f\"  Mean P(tool_used): {mean_prob_tool_used:.3f}\")\n",
    "\n",
    "if correct_on_fakes > 0.95:\n",
    "    print(\"  ✓ Probe aligns with reality, not narrative!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Histogram of P(tool_used) on fake vs true cases\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# Get predictions on true action cases for comparison\n",
    "true_analysis = analyze_probe_on_category(\n",
    "    reality_probe,\n",
    "    probe_dataset,\n",
    "    category=\"true_action\",\n",
    "    label_type=\"reality\",\n",
    ")\n",
    "\n",
    "# Plot fake action probabilities\n",
    "axes[0].hist(fake_probs, bins=20, alpha=0.7, color='red', edgecolor='black')\n",
    "axes[0].axvline(x=0.5, color='k', linestyle='--', linewidth=1)\n",
    "axes[0].set_xlabel('P(tool_used)')\n",
    "axes[0].set_ylabel('Count')\n",
    "axes[0].set_title(f'Fake Actions (should cluster near 0)\\nMean = {mean_prob_tool_used:.3f}')\n",
    "axes[0].set_xlim(0, 1)\n",
    "\n",
    "# Plot true action probabilities\n",
    "true_probs = true_analysis['probabilities']\n",
    "mean_prob_true = np.mean(true_probs)\n",
    "axes[1].hist(true_probs, bins=20, alpha=0.7, color='green', edgecolor='black')\n",
    "axes[1].axvline(x=0.5, color='k', linestyle='--', linewidth=1)\n",
    "axes[1].set_xlabel('P(tool_used)')\n",
    "axes[1].set_ylabel('Count')\n",
    "axes[1].set_title(f'True Actions (should cluster near 1)\\nMean = {mean_prob_true:.3f}')\n",
    "axes[1].set_xlim(0, 1)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(config.data.figures_dir / \"figure3_fake_vs_true_probs.png\", dpi=300, bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Layer Analysis\n",
    "\n",
    "Which layers encode action-grounding information?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train probes at each layer (using mid_response position)\n",
    "layer_accuracies = {}\n",
    "\n",
    "for layer in config.extraction.layers:\n",
    "    print(f\"\\nTraining probe at layer: {layer}\")\n",
    "    \n",
    "    layer_dataset = dataset.filter_by_position(\"mid_response\").filter_by_layer(layer)\n",
    "    \n",
    "    if len(layer_dataset) == 0:\n",
    "        print(f\"  No samples found for layer {layer}\")\n",
    "        continue\n",
    "    \n",
    "    probe, _, test_metrics = train_and_evaluate(\n",
    "        layer_dataset,\n",
    "        label_type=\"reality\",\n",
    "        random_state=42,\n",
    "    )\n",
    "    \n",
    "    layer_accuracies[layer] = test_metrics.accuracy\n",
    "    print(f\"  Test accuracy: {test_metrics.accuracy:.1%}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize layer analysis\n",
    "fig = plot_layer_analysis(\n",
    "    layer_accuracies,\n",
    "    title=\"Reality Probe Accuracy by Layer\",\n",
    "    save_path=config.data.figures_dir / \"figure5_layer_accuracy\",\n",
    ")\n",
    "\n",
    "plt.show()\n",
    "\n",
    "# Best layer\n",
    "best_layer = max(layer_accuracies.items(), key=lambda x: x[1])\n",
    "print(f\"\\nBest layer: {best_layer[0]} (accuracy: {best_layer[1]:.1%})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Probe Direction Analysis\n",
    "\n",
    "Are reality and narrative probes learning the same representation?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare probe directions\n",
    "from src.analysis.probes import compare_probes\n",
    "\n",
    "comparison = compare_probes(reality_probe, narrative_probe, normalize=True)\n",
    "\n",
    "print(f\"\\nReality vs Narrative Probe:\")\n",
    "print(f\"  Cosine similarity: {comparison['cosine_similarity']:.3f}\")\n",
    "print(f\"  L2 distance: {comparison['l2_distance']:.3f}\")\n",
    "\n",
    "if abs(comparison['cosine_similarity']) > 0.8:\n",
    "    print(\"  → Probes learn similar directions (aligned)\")\n",
    "elif abs(comparison['cosine_similarity']) < 0.3:\n",
    "    print(\"  → Probes learn different directions (independent representations)\")\n",
    "else:\n",
    "    print(\"  → Probes partially aligned\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Visualizations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get test data for reality probe\n",
    "train_dataset, test_dataset = probe_dataset.train_test_split(test_size=0.2, random_state=42)\n",
    "X_test, y_test = test_dataset.to_sklearn_format(\"reality\")\n",
    "\n",
    "# Confusion matrix\n",
    "y_pred = reality_probe.predict(X_test)\n",
    "fig = plot_confusion_matrix(\n",
    "    y_test,\n",
    "    y_pred,\n",
    "    labels=[\"No Tool\", \"Tool Used\"],\n",
    "    title=\"Reality Probe Confusion Matrix\",\n",
    "    save_path=config.data.figures_dir / \"reality_probe_confusion\",\n",
    ")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ROC curve\n",
    "y_proba = reality_probe.predict_proba(X_test)[:, 1]\n",
    "auc, fpr, tpr, thresholds = compute_roc_auc(y_test, y_proba)\n",
    "\n",
    "fig = plot_roc_curve(\n",
    "    fpr,\n",
    "    tpr,\n",
    "    auc,\n",
    "    title=\"Reality Probe ROC Curve\",\n",
    "    save_path=config.data.figures_dir / \"reality_probe_roc\",\n",
    ")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\" * 60)\n",
    "print(\"PHASE 2 RESULTS: MECHANISTIC PROBES\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "print(f\"\\nReality Probe Performance:\")\n",
    "print(f\"  Test Accuracy: {reality_test_metrics.accuracy:.1%}\")\n",
    "print(f\"  ROC-AUC: {reality_test_metrics.roc_auc:.3f}\")\n",
    "\n",
    "print(f\"\\nPosition Analysis:\")\n",
    "for pos, acc in position_accuracies.items():\n",
    "    print(f\"  {pos}: {acc:.1%}\")\n",
    "\n",
    "print(f\"\\nFake Action Analysis:\")\n",
    "print(f\"  Correct on fakes: {correct_on_fakes:.1%}\")\n",
    "print(f\"  Mean P(tool_used) on fakes: {mean_prob_tool_used:.3f}\")\n",
    "\n",
    "print(f\"\\nProbe Direction Comparison:\")\n",
    "print(f\"  Cosine similarity: {comparison['cosine_similarity']:.3f}\")\n",
    "\n",
    "print(\"\\n✓ Phase 2 complete: Linear probe can detect ground truth\")\n",
    "print(\"=\" * 60)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Next Steps\n",
    "\n",
    "→ **Notebook 03:** Test cross-tool generalization"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
