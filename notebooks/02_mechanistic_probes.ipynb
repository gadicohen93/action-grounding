{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Notebook 02: Mechanistic Probes\n",
    "\n",
    "**Research Question:** Can we detect ground truth action-taking from model activations?\n",
    "\n",
    "This notebook:\n",
    "1. Extracts activations at multiple positions and layers\n",
    "2. Trains reality (tool_used) and narrative (claims_action) probes\n",
    "3. **Critical:** Position analysis (first_assistant vs before_tool)\n",
    "4. Analyzes probe behavior on fake action cases\n",
    "\n",
    "**Key hypothesis:** If `first_assistant` accuracy > 80%, probe is detecting action-grounding, not just tool syntax."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extraction config:\n",
      "  Positions: ['first_assistant', 'mid_response', 'before_tool']\n",
      "  Layers: [0, 8, 16, 24, 31]\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "sys.path.append('..')\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from pathlib import Path\n",
    "\n",
    "from src.utils.logging import setup_logging\n",
    "from src.config import get_config\n",
    "from src.data.io import load_episodes, save_activations, load_activations\n",
    "from src.extraction import extract_activations_batch\n",
    "from src.analysis.probes import train_and_evaluate, analyze_probe_on_category, save_probe\n",
    "from src.analysis.visualization import (\n",
    "    plot_confusion_matrix,\n",
    "    plot_roc_curve,\n",
    "    plot_position_accuracy,\n",
    "    plot_layer_analysis,\n",
    ")\n",
    "from src.analysis.statistics import compute_roc_auc, bootstrap_metrics\n",
    "\n",
    "setup_logging(level=\"INFO\")\n",
    "config = get_config()\n",
    "\n",
    "print(f\"Extraction config:\")\n",
    "print(f\"  Positions: {config.extraction.positions}\")\n",
    "print(f\"  Layers: {config.extraction.layers}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Load Episodes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-12-24 06:22:21,272 - src.data.io - INFO - Loading episodes from: data/processed/new_episodes.parquet\n",
      "2025-12-24 06:22:21,334 - src.data.io - INFO - Loaded 360 episodes\n",
      "2025-12-24 06:22:21,335 - src.data.io - INFO - Loading episodes from: data/processed/episodes.parquet\n",
      "2025-12-24 06:22:21,390 - src.data.io - INFO - Loaded 360 episodes\n",
      "Loaded 720 episodes\n",
      "\n",
      "Category breakdown:\n",
      "  honest_no_action: 80\n",
      "  silent_action: 241\n",
      "  true_action: 32\n",
      "  fake_action: 7\n"
     ]
    }
   ],
   "source": [
    "from src.data.episode import EpisodeCollection\n",
    "\n",
    "# episodes_collection = load_episodes(config.data.processed_dir / \"episodes.parquet\")\n",
    "new_episodes = load_episodes(\"data/processed/new_episodes.parquet\")\n",
    "old_episodes = load_episodes(\"data/processed/episodes.parquet\")\n",
    "\n",
    "# Combine all episodes\n",
    "all_episodes = EpisodeCollection(\n",
    "    episodes=new_episodes.episodes + old_episodes.episodes,\n",
    "    description=\"Combined episodes\"\n",
    ")\n",
    "\n",
    "episodes = all_episodes.episodes\n",
    "\n",
    "\n",
    "print(f\"Loaded {len(episodes)} episodes\")\n",
    "print(f\"\\nCategory breakdown:\")\n",
    "summary = episodes_collection.summary()\n",
    "for cat, count in summary['categories'].items():\n",
    "    print(f\"  {cat}: {count}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Extract Activations\n",
    "\n",
    "Extract at 3 positions × 5 layers = 15 samples per episode.\n",
    "\n",
    "**WARNING:** This takes 1-2 hours on GPU depending on model size."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-12-24 06:22:23,697 - src.backends.pytorch - INFO - Loading model: mistralai/Mistral-7B-Instruct-v0.2\n",
      "2025-12-24 06:22:23,697 - src.backends.pytorch - INFO -   Quantization: 8bit\n",
      "2025-12-24 06:22:23,698 - src.backends.pytorch - INFO -   Device map: auto\n",
      "2025-12-24 06:22:23,699 - src.backends.pytorch - INFO -   Dtype: float16\n",
      "2025-12-24 06:22:23,699 - src.backends.pytorch - INFO -   Using 8-bit quantization\n",
      "2025-12-24 06:22:24,507 - accelerate.utils.modeling - INFO - We will use 90% of the memory on device 0 for storing the model, and 10% for the buffer to avoid OOM. You can set `max_memory` in to a higher value to use more memory (at your own risk).\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5cdd760b58f3465fac763fe176626aeb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-12-24 06:22:37,140 - src.backends.pytorch - INFO - Model loaded. Parameters: 7,241,732,096\n",
      "2025-12-24 06:22:37,141 - src.extraction.activations - INFO - Initialized ActivationExtractor:\n",
      "2025-12-24 06:22:37,142 - src.extraction.activations - INFO -   Model: mistralai/Mistral-7B-Instruct-v0.2\n",
      "2025-12-24 06:22:37,143 - src.extraction.activations - INFO -   Hidden size: 4096\n",
      "2025-12-24 06:22:37,145 - src.extraction.activations - INFO -   Layers: 32\n",
      "2025-12-24 06:22:37,145 - src.extraction.activations - INFO - Extracting activations from 720 episodes\n",
      "2025-12-24 06:22:37,146 - src.extraction.activations - INFO -   Positions: ['first_assistant', 'mid_response', 'before_tool']\n",
      "2025-12-24 06:22:37,147 - src.extraction.activations - INFO -   Layers: [0, 8, 16, 24, 31]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Extracting:   0%|          | 0/720 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-12-24 06:22:37,151 - src.backends.pytorch - INFO - Loading model: mistralai/Mistral-7B-Instruct-v0.2\n",
      "2025-12-24 06:22:37,151 - src.backends.pytorch - INFO -   Quantization: 8bit\n",
      "2025-12-24 06:22:37,153 - src.backends.pytorch - INFO -   Device map: auto\n",
      "2025-12-24 06:22:37,153 - src.backends.pytorch - INFO -   Dtype: float16\n",
      "2025-12-24 06:22:37,154 - src.backends.pytorch - INFO -   Using 8-bit quantization\n",
      "2025-12-24 06:22:37,324 - accelerate.utils.modeling - INFO - We will use 90% of the memory on device 0 for storing the model, and 10% for the buffer to avoid OOM. You can set `max_memory` in to a higher value to use more memory (at your own risk).\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a4cd2af88c6f4629bd6a4bd850640187",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-12-24 06:22:49,184 - src.backends.pytorch - INFO - Model loaded. Parameters: 7,241,732,096\n",
      "2025-12-24 06:22:49,214 - src.backends.pytorch - INFO - Loading model: mistralai/Mistral-7B-Instruct-v0.2\n",
      "2025-12-24 06:22:49,215 - src.backends.pytorch - INFO -   Quantization: 8bit\n",
      "2025-12-24 06:22:49,215 - src.backends.pytorch - INFO -   Device map: auto\n",
      "2025-12-24 06:22:49,216 - src.backends.pytorch - INFO -   Dtype: float16\n",
      "2025-12-24 06:22:49,216 - src.backends.pytorch - INFO -   Using 8-bit quantization\n",
      "2025-12-24 06:22:49,363 - accelerate.utils.modeling - INFO - We will use 90% of the memory on device 0 for storing the model, and 10% for the buffer to avoid OOM. You can set `max_memory` in to a higher value to use more memory (at your own risk).\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "70ceeb3e992b47b4a8cd79bdaa1ba556",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-12-24 06:23:01,085 - src.backends.pytorch - INFO - Model loaded. Parameters: 7,241,732,096\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Extracting:   0%|          | 1/720 [00:24<4:50:41, 24.26s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-12-24 06:23:01,410 - src.backends.pytorch - INFO - Loading model: mistralai/Mistral-7B-Instruct-v0.2\n",
      "2025-12-24 06:23:01,410 - src.backends.pytorch - INFO -   Quantization: 8bit\n",
      "2025-12-24 06:23:01,411 - src.backends.pytorch - INFO -   Device map: auto\n",
      "2025-12-24 06:23:01,411 - src.backends.pytorch - INFO -   Dtype: float16\n",
      "2025-12-24 06:23:01,412 - src.backends.pytorch - INFO -   Using 8-bit quantization\n",
      "2025-12-24 06:23:01,548 - accelerate.utils.modeling - INFO - We will use 90% of the memory on device 0 for storing the model, and 10% for the buffer to avoid OOM. You can set `max_memory` in to a higher value to use more memory (at your own risk).\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8ae9196e19c549058ef1e6f737cf29e9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-12-24 06:23:13,347 - src.backends.pytorch - INFO - Model loaded. Parameters: 7,241,732,096\n",
      "2025-12-24 06:23:13,355 - src.backends.pytorch - INFO - Loading model: mistralai/Mistral-7B-Instruct-v0.2\n",
      "2025-12-24 06:23:13,355 - src.backends.pytorch - INFO -   Quantization: 8bit\n",
      "2025-12-24 06:23:13,356 - src.backends.pytorch - INFO -   Device map: auto\n",
      "2025-12-24 06:23:13,357 - src.backends.pytorch - INFO -   Dtype: float16\n",
      "2025-12-24 06:23:13,358 - src.backends.pytorch - INFO -   Using 8-bit quantization\n",
      "2025-12-24 06:23:13,548 - accelerate.utils.modeling - INFO - We will use 90% of the memory on device 0 for storing the model, and 10% for the buffer to avoid OOM. You can set `max_memory` in to a higher value to use more memory (at your own risk).\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dc082f2a274f41d8b97d49132f4f06c9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-12-24 06:23:25,083 - src.backends.pytorch - INFO - Model loaded. Parameters: 7,241,732,096\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Extracting:   0%|          | 2/720 [00:48<4:48:23, 24.10s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-12-24 06:23:25,399 - src.backends.pytorch - INFO - Loading model: mistralai/Mistral-7B-Instruct-v0.2\n",
      "2025-12-24 06:23:25,399 - src.backends.pytorch - INFO -   Quantization: 8bit\n",
      "2025-12-24 06:23:25,400 - src.backends.pytorch - INFO -   Device map: auto\n",
      "2025-12-24 06:23:25,400 - src.backends.pytorch - INFO -   Dtype: float16\n",
      "2025-12-24 06:23:25,400 - src.backends.pytorch - INFO -   Using 8-bit quantization\n",
      "2025-12-24 06:23:25,568 - accelerate.utils.modeling - INFO - We will use 90% of the memory on device 0 for storing the model, and 10% for the buffer to avoid OOM. You can set `max_memory` in to a higher value to use more memory (at your own risk).\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1ff78c88ddeb42b9be87bf99eb2b3bc4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-12-24 06:23:36,668 - src.backends.pytorch - INFO - Model loaded. Parameters: 7,241,732,096\n",
      "2025-12-24 06:23:36,676 - src.backends.pytorch - INFO - Loading model: mistralai/Mistral-7B-Instruct-v0.2\n",
      "2025-12-24 06:23:36,676 - src.backends.pytorch - INFO -   Quantization: 8bit\n",
      "2025-12-24 06:23:36,676 - src.backends.pytorch - INFO -   Device map: auto\n",
      "2025-12-24 06:23:36,677 - src.backends.pytorch - INFO -   Dtype: float16\n",
      "2025-12-24 06:23:36,677 - src.backends.pytorch - INFO -   Using 8-bit quantization\n",
      "2025-12-24 06:23:36,833 - accelerate.utils.modeling - INFO - We will use 90% of the memory on device 0 for storing the model, and 10% for the buffer to avoid OOM. You can set `max_memory` in to a higher value to use more memory (at your own risk).\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1ef714fd7ef6465b84afc70c2171af14",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-12-24 06:23:48,095 - src.backends.pytorch - INFO - Model loaded. Parameters: 7,241,732,096\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Extracting:   0%|          | 3/720 [01:11<4:42:05, 23.61s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-12-24 06:23:48,417 - src.backends.pytorch - INFO - Loading model: mistralai/Mistral-7B-Instruct-v0.2\n",
      "2025-12-24 06:23:48,417 - src.backends.pytorch - INFO -   Quantization: 8bit\n",
      "2025-12-24 06:23:48,418 - src.backends.pytorch - INFO -   Device map: auto\n",
      "2025-12-24 06:23:48,418 - src.backends.pytorch - INFO -   Dtype: float16\n",
      "2025-12-24 06:23:48,419 - src.backends.pytorch - INFO -   Using 8-bit quantization\n",
      "2025-12-24 06:23:48,579 - accelerate.utils.modeling - INFO - We will use 90% of the memory on device 0 for storing the model, and 10% for the buffer to avoid OOM. You can set `max_memory` in to a higher value to use more memory (at your own risk).\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cf69f86db7b24640b35da0144632321d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-12-24 06:23:59,932 - src.backends.pytorch - INFO - Model loaded. Parameters: 7,241,732,096\n",
      "2025-12-24 06:23:59,938 - src.backends.pytorch - INFO - Loading model: mistralai/Mistral-7B-Instruct-v0.2\n",
      "2025-12-24 06:23:59,939 - src.backends.pytorch - INFO -   Quantization: 8bit\n",
      "2025-12-24 06:23:59,939 - src.backends.pytorch - INFO -   Device map: auto\n",
      "2025-12-24 06:23:59,939 - src.backends.pytorch - INFO -   Dtype: float16\n",
      "2025-12-24 06:23:59,940 - src.backends.pytorch - INFO -   Using 8-bit quantization\n",
      "2025-12-24 06:24:00,066 - accelerate.utils.modeling - INFO - We will use 90% of the memory on device 0 for storing the model, and 10% for the buffer to avoid OOM. You can set `max_memory` in to a higher value to use more memory (at your own risk).\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "32bc0a676a0545f3ab771dac27d8f0a6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-12-24 06:24:10,976 - src.backends.pytorch - INFO - Model loaded. Parameters: 7,241,732,096\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Extracting:   1%|          | 4/720 [01:34<4:38:10, 23.31s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-12-24 06:24:11,274 - src.backends.pytorch - INFO - Loading model: mistralai/Mistral-7B-Instruct-v0.2\n",
      "2025-12-24 06:24:11,274 - src.backends.pytorch - INFO -   Quantization: 8bit\n",
      "2025-12-24 06:24:11,275 - src.backends.pytorch - INFO -   Device map: auto\n",
      "2025-12-24 06:24:11,275 - src.backends.pytorch - INFO -   Dtype: float16\n",
      "2025-12-24 06:24:11,276 - src.backends.pytorch - INFO -   Using 8-bit quantization\n",
      "2025-12-24 06:24:11,428 - accelerate.utils.modeling - INFO - We will use 90% of the memory on device 0 for storing the model, and 10% for the buffer to avoid OOM. You can set `max_memory` in to a higher value to use more memory (at your own risk).\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e30ba459d0ce418a93f188edf8e4f76c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-12-24 06:24:22,880 - src.backends.pytorch - INFO - Model loaded. Parameters: 7,241,732,096\n",
      "2025-12-24 06:24:22,888 - src.backends.pytorch - INFO - Loading model: mistralai/Mistral-7B-Instruct-v0.2\n",
      "2025-12-24 06:24:22,890 - src.backends.pytorch - INFO -   Quantization: 8bit\n",
      "2025-12-24 06:24:22,891 - src.backends.pytorch - INFO -   Device map: auto\n",
      "2025-12-24 06:24:22,892 - src.backends.pytorch - INFO -   Dtype: float16\n",
      "2025-12-24 06:24:22,893 - src.backends.pytorch - INFO -   Using 8-bit quantization\n",
      "2025-12-24 06:24:23,273 - accelerate.utils.modeling - INFO - We will use 90% of the memory on device 0 for storing the model, and 10% for the buffer to avoid OOM. You can set `max_memory` in to a higher value to use more memory (at your own risk).\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "421f604caedd48769f4add99748af7bf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-12-24 06:24:34,537 - src.backends.pytorch - INFO - Model loaded. Parameters: 7,241,732,096\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Extracting:   1%|          | 5/720 [01:57<4:39:04, 23.42s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-12-24 06:24:34,886 - src.backends.pytorch - INFO - Loading model: mistralai/Mistral-7B-Instruct-v0.2\n",
      "2025-12-24 06:24:34,887 - src.backends.pytorch - INFO -   Quantization: 8bit\n",
      "2025-12-24 06:24:34,888 - src.backends.pytorch - INFO -   Device map: auto\n",
      "2025-12-24 06:24:34,889 - src.backends.pytorch - INFO -   Dtype: float16\n",
      "2025-12-24 06:24:34,889 - src.backends.pytorch - INFO -   Using 8-bit quantization\n",
      "2025-12-24 06:24:35,147 - accelerate.utils.modeling - INFO - We will use 90% of the memory on device 0 for storing the model, and 10% for the buffer to avoid OOM. You can set `max_memory` in to a higher value to use more memory (at your own risk).\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fe45a05a48ee48e0976a8d334273cd65",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Extracting:   1%|          | 5/720 [02:09<5:08:05, 25.85s/it]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[11]\u001b[39m\u001b[32m, line 2\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;66;03m# Extract activations\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m2\u001b[39m dataset = \u001b[43mextract_activations_batch\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m      3\u001b[39m \u001b[43m    \u001b[49m\u001b[43mepisodes\u001b[49m\u001b[43m=\u001b[49m\u001b[43mepisodes\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m      4\u001b[39m \u001b[43m    \u001b[49m\u001b[43mpositions\u001b[49m\u001b[43m=\u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m.\u001b[49m\u001b[43mextraction\u001b[49m\u001b[43m.\u001b[49m\u001b[43mpositions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m      5\u001b[39m \u001b[43m    \u001b[49m\u001b[43mlayers\u001b[49m\u001b[43m=\u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m.\u001b[49m\u001b[43mextraction\u001b[49m\u001b[43m.\u001b[49m\u001b[43mlayers\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m      6\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmodel_id\u001b[49m\u001b[43m=\u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m.\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m.\u001b[49m\u001b[43mid\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m      7\u001b[39m \u001b[43m    \u001b[49m\u001b[43msave_path\u001b[49m\u001b[43m=\u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m.\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m.\u001b[49m\u001b[43mprocessed_dir\u001b[49m\u001b[43m \u001b[49m\u001b[43m/\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mactivations.parquet\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m      8\u001b[39m \u001b[43m    \u001b[49m\u001b[43mverbose\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m      9\u001b[39m \u001b[43m)\u001b[49m\n\u001b[32m     11\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33mExtracted \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlen\u001b[39m(dataset)\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m activation samples\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m     12\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mActivation shape: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mdataset.activations.shape\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/workspace/action-grounding/notebooks/../src/extraction/activations.py:332\u001b[39m, in \u001b[36mextract_activations_batch\u001b[39m\u001b[34m(episodes, positions, layers, model_id, backend_type, save_path, verbose)\u001b[39m\n\u001b[32m    320\u001b[39m         extractor.unload()\n\u001b[32m    322\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m samples\n\u001b[32m    325\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mextract_activations_batch\u001b[39m(\n\u001b[32m    326\u001b[39m     episodes: \u001b[38;5;28mlist\u001b[39m[Episode],\n\u001b[32m    327\u001b[39m     positions: Optional[\u001b[38;5;28mlist\u001b[39m[\u001b[38;5;28mstr\u001b[39m]] = \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[32m    328\u001b[39m     layers: Optional[\u001b[38;5;28mlist\u001b[39m[\u001b[38;5;28mint\u001b[39m]] = \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[32m    329\u001b[39m     model_id: Optional[\u001b[38;5;28mstr\u001b[39m] = \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[32m    330\u001b[39m     backend_type: Optional[\u001b[38;5;28mstr\u001b[39m] = \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[32m    331\u001b[39m     save_path: Optional[\u001b[38;5;28mstr\u001b[39m] = \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[32m--> \u001b[39m\u001b[32m332\u001b[39m     verbose: \u001b[38;5;28mbool\u001b[39m = \u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[32m    333\u001b[39m ) -> ActivationDataset:\n\u001b[32m    334\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m    335\u001b[39m \u001b[33;03m    Extract activations from a batch of episodes (convenience function).\u001b[39;00m\n\u001b[32m    336\u001b[39m \n\u001b[32m   (...)\u001b[39m\u001b[32m    352\u001b[39m \u001b[33;03m        ActivationDataset\u001b[39;00m\n\u001b[32m    353\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m    354\u001b[39m     config = get_config()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/workspace/action-grounding/notebooks/../src/extraction/activations.py:189\u001b[39m, in \u001b[36mActivationExtractor.extract_batch\u001b[39m\u001b[34m(self, episodes, positions, layers, verbose)\u001b[39m\n\u001b[32m    187\u001b[39m logger.info(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33m  Started at: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mdatetime.now().strftime(\u001b[33m'\u001b[39m\u001b[33m%\u001b[39m\u001b[33mH:\u001b[39m\u001b[33m%\u001b[39m\u001b[33mM:\u001b[39m\u001b[33m%\u001b[39m\u001b[33mS\u001b[39m\u001b[33m'\u001b[39m)\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n\u001b[32m    188\u001b[39m logger.info(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33m  Episodes: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlen\u001b[39m(episodes)\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n\u001b[32m--> \u001b[39m\u001b[32m189\u001b[39m logger.info(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33m  Positions: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mpositions\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n\u001b[32m    190\u001b[39m logger.info(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33m  Layers: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mlayers\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n\u001b[32m    191\u001b[39m logger.info(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33m  Expected samples: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlen\u001b[39m(episodes)\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m × \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlen\u001b[39m(positions)\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m × \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlen\u001b[39m(layers)\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m = \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlen\u001b[39m(episodes)\u001b[38;5;250m \u001b[39m*\u001b[38;5;250m \u001b[39m\u001b[38;5;28mlen\u001b[39m(positions)\u001b[38;5;250m \u001b[39m*\u001b[38;5;250m \u001b[39m\u001b[38;5;28mlen\u001b[39m(layers)\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/workspace/action-grounding/notebooks/../src/extraction/activations.py:92\u001b[39m, in \u001b[36mActivationExtractor.extract\u001b[39m\u001b[34m(self, episode, positions, layers)\u001b[39m\n\u001b[32m     85\u001b[39m full_text = \u001b[38;5;28mself\u001b[39m.backend.format_chat(\n\u001b[32m     86\u001b[39m     episode.system_prompt,\n\u001b[32m     87\u001b[39m     episode.user_turns,\n\u001b[32m     88\u001b[39m     assistant_prefix=episode.assistant_reply,\n\u001b[32m     89\u001b[39m )\n\u001b[32m     91\u001b[39m \u001b[38;5;66;03m# Find token positions\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m92\u001b[39m position_objs = \u001b[43mfind_all_positions\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m     93\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mbackend\u001b[49m\u001b[43m.\u001b[49m\u001b[43mtokenizer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     94\u001b[39m \u001b[43m    \u001b[49m\u001b[43mfull_text\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     95\u001b[39m \u001b[43m    \u001b[49m\u001b[43mepisode\u001b[49m\u001b[43m.\u001b[49m\u001b[43msystem_prompt\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     96\u001b[39m \u001b[43m    \u001b[49m\u001b[43mepisode\u001b[49m\u001b[43m.\u001b[49m\u001b[43muser_turns\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     97\u001b[39m \u001b[43m    \u001b[49m\u001b[43mposition_names\u001b[49m\u001b[43m=\u001b[49m\u001b[43mpositions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     98\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    100\u001b[39m \u001b[38;5;66;03m# Extract hidden states for all layers\u001b[39;00m\n\u001b[32m    101\u001b[39m hidden_states = \u001b[38;5;28mself\u001b[39m.backend.get_hidden_states(full_text, layers=layers)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/workspace/action-grounding/notebooks/../src/extraction/positions.py:275\u001b[39m, in \u001b[36mfind_all_positions\u001b[39m\u001b[34m(tokenizer, full_text, system_prompt, user_turns, position_names)\u001b[39m\n\u001b[32m    273\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m name \u001b[38;5;129;01min\u001b[39;00m position_names:\n\u001b[32m    274\u001b[39m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m275\u001b[39m         pos = \u001b[43mfind_token_position\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    276\u001b[39m \u001b[43m            \u001b[49m\u001b[43mtokenizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfull_text\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msystem_prompt\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43muser_turns\u001b[49m\n\u001b[32m    277\u001b[39m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    278\u001b[39m         positions[name] = pos\n\u001b[32m    279\u001b[39m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/workspace/action-grounding/notebooks/../src/extraction/positions.py:231\u001b[39m, in \u001b[36mfind_token_position\u001b[39m\u001b[34m(tokenizer, full_text, position_name, system_prompt, user_turns)\u001b[39m\n\u001b[32m    229\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m system_prompt \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mor\u001b[39;00m user_turns \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    230\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[33m\"\u001b[39m\u001b[33msystem_prompt and user_turns required for first_assistant\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m--> \u001b[39m\u001b[32m231\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfind_first_assistant_position\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtokenizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfull_text\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msystem_prompt\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43muser_turns\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    233\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m position_name == \u001b[33m\"\u001b[39m\u001b[33mmid_response\u001b[39m\u001b[33m\"\u001b[39m:\n\u001b[32m    234\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m system_prompt \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mor\u001b[39;00m user_turns \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/workspace/action-grounding/notebooks/../src/extraction/positions.py:72\u001b[39m, in \u001b[36mfind_first_assistant_position\u001b[39m\u001b[34m(tokenizer, full_text, system_prompt, user_turns)\u001b[39m\n\u001b[32m     66\u001b[39m backend_class = get_backend(config.model.backend)\n\u001b[32m     67\u001b[39m backend = backend_class(\n\u001b[32m     68\u001b[39m     model_id=config.model.id,\n\u001b[32m     69\u001b[39m     quantization=config.model.quantization,\n\u001b[32m     70\u001b[39m )\n\u001b[32m---> \u001b[39m\u001b[32m72\u001b[39m prompt_only = \u001b[43mbackend\u001b[49m\u001b[43m.\u001b[49m\u001b[43mformat_chat\u001b[49m\u001b[43m(\u001b[49m\u001b[43msystem_prompt\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43muser_turns\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     73\u001b[39m prompt_ids = tokenizer.encode(prompt_only, add_special_tokens=\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[32m     75\u001b[39m \u001b[38;5;66;03m# First assistant token is right after prompt\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/workspace/action-grounding/notebooks/../src/backends/pytorch.py:403\u001b[39m, in \u001b[36mPyTorchBackend.format_chat\u001b[39m\u001b[34m(self, system_prompt, user_turns, assistant_prefix)\u001b[39m\n\u001b[32m    400\u001b[39m         messages.append({\u001b[33m\"\u001b[39m\u001b[33mrole\u001b[39m\u001b[33m\"\u001b[39m: \u001b[33m\"\u001b[39m\u001b[33massistant\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33mcontent\u001b[39m\u001b[33m\"\u001b[39m: \u001b[33m\"\u001b[39m\u001b[33m\"\u001b[39m})\n\u001b[32m    402\u001b[39m \u001b[38;5;66;03m# Try using the tokenizer's chat template\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m403\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mtokenizer\u001b[49m, \u001b[33m\"\u001b[39m\u001b[33mapply_chat_template\u001b[39m\u001b[33m\"\u001b[39m):\n\u001b[32m    404\u001b[39m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m    405\u001b[39m         prompt = \u001b[38;5;28mself\u001b[39m.tokenizer.apply_chat_template(\n\u001b[32m    406\u001b[39m             messages,\n\u001b[32m    407\u001b[39m             tokenize=\u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[32m    408\u001b[39m             add_generation_prompt=\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[32m    409\u001b[39m         )\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/workspace/action-grounding/notebooks/../src/backends/base.py:68\u001b[39m, in \u001b[36mModelBackend.tokenizer\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m     66\u001b[39m \u001b[38;5;250m\u001b[39m\u001b[33;03m\"\"\"Get the tokenizer (lazy loaded).\"\"\"\u001b[39;00m\n\u001b[32m     67\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._tokenizer \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m---> \u001b[39m\u001b[32m68\u001b[39m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_load\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     69\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._tokenizer\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/workspace/action-grounding/notebooks/../src/backends/pytorch.py:70\u001b[39m, in \u001b[36mPyTorchBackend._load\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m     67\u001b[39m     load_kwargs[\u001b[33m\"\u001b[39m\u001b[33mtorch_dtype\u001b[39m\u001b[33m\"\u001b[39m] = torch_dtype\n\u001b[32m     69\u001b[39m \u001b[38;5;66;03m# Load model\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m70\u001b[39m \u001b[38;5;28mself\u001b[39m._model = \u001b[43mAutoModelForCausalLM\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfrom_pretrained\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m     71\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mmodel_id\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     72\u001b[39m \u001b[43m    \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mload_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     73\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     75\u001b[39m \u001b[38;5;66;03m# Load tokenizer\u001b[39;00m\n\u001b[32m     76\u001b[39m \u001b[38;5;28mself\u001b[39m._tokenizer = AutoTokenizer.from_pretrained(\n\u001b[32m     77\u001b[39m     \u001b[38;5;28mself\u001b[39m.model_id,\n\u001b[32m     78\u001b[39m     trust_remote_code=\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[32m     79\u001b[39m )\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/workspace/action-grounding/venv/lib/python3.11/site-packages/transformers/models/auto/auto_factory.py:604\u001b[39m, in \u001b[36m_BaseAutoModelClass.from_pretrained\u001b[39m\u001b[34m(cls, pretrained_model_name_or_path, *model_args, **kwargs)\u001b[39m\n\u001b[32m    602\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m model_class.config_class == config.sub_configs.get(\u001b[33m\"\u001b[39m\u001b[33mtext_config\u001b[39m\u001b[33m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[32m    603\u001b[39m         config = config.get_text_config()\n\u001b[32m--> \u001b[39m\u001b[32m604\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mmodel_class\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfrom_pretrained\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    605\u001b[39m \u001b[43m        \u001b[49m\u001b[43mpretrained_model_name_or_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43mmodel_args\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m=\u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mhub_kwargs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\n\u001b[32m    606\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    607\u001b[39m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[32m    608\u001b[39m     \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mUnrecognized configuration class \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mconfig.\u001b[34m__class__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m for this kind of AutoModel: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mcls\u001b[39m.\u001b[34m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m\"\u001b[39m\n\u001b[32m    609\u001b[39m     \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mModel type should be one of \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[33m'\u001b[39m\u001b[33m, \u001b[39m\u001b[33m'\u001b[39m.join(c.\u001b[34m__name__\u001b[39m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mfor\u001b[39;00m\u001b[38;5;250m \u001b[39mc\u001b[38;5;250m \u001b[39m\u001b[38;5;129;01min\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28mcls\u001b[39m._model_mapping)\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m.\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    610\u001b[39m )\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/workspace/action-grounding/venv/lib/python3.11/site-packages/transformers/modeling_utils.py:277\u001b[39m, in \u001b[36mrestore_default_dtype.<locals>._wrapper\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    275\u001b[39m old_dtype = torch.get_default_dtype()\n\u001b[32m    276\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m277\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    278\u001b[39m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[32m    279\u001b[39m     torch.set_default_dtype(old_dtype)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/workspace/action-grounding/venv/lib/python3.11/site-packages/transformers/modeling_utils.py:5048\u001b[39m, in \u001b[36mPreTrainedModel.from_pretrained\u001b[39m\u001b[34m(cls, pretrained_model_name_or_path, config, cache_dir, ignore_mismatched_sizes, force_download, local_files_only, token, revision, use_safetensors, weights_only, *model_args, **kwargs)\u001b[39m\n\u001b[32m   5038\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m dtype_orig \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m   5039\u001b[39m         torch.set_default_dtype(dtype_orig)\n\u001b[32m   5041\u001b[39m     (\n\u001b[32m   5042\u001b[39m         model,\n\u001b[32m   5043\u001b[39m         missing_keys,\n\u001b[32m   5044\u001b[39m         unexpected_keys,\n\u001b[32m   5045\u001b[39m         mismatched_keys,\n\u001b[32m   5046\u001b[39m         offload_index,\n\u001b[32m   5047\u001b[39m         error_msgs,\n\u001b[32m-> \u001b[39m\u001b[32m5048\u001b[39m     ) = \u001b[38;5;28;43mcls\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_load_pretrained_model\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   5049\u001b[39m \u001b[43m        \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   5050\u001b[39m \u001b[43m        \u001b[49m\u001b[43mstate_dict\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   5051\u001b[39m \u001b[43m        \u001b[49m\u001b[43mcheckpoint_files\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   5052\u001b[39m \u001b[43m        \u001b[49m\u001b[43mpretrained_model_name_or_path\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   5053\u001b[39m \u001b[43m        \u001b[49m\u001b[43mignore_mismatched_sizes\u001b[49m\u001b[43m=\u001b[49m\u001b[43mignore_mismatched_sizes\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   5054\u001b[39m \u001b[43m        \u001b[49m\u001b[43msharded_metadata\u001b[49m\u001b[43m=\u001b[49m\u001b[43msharded_metadata\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   5055\u001b[39m \u001b[43m        \u001b[49m\u001b[43mdevice_map\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdevice_map\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   5056\u001b[39m \u001b[43m        \u001b[49m\u001b[43mdisk_offload_folder\u001b[49m\u001b[43m=\u001b[49m\u001b[43moffload_folder\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   5057\u001b[39m \u001b[43m        \u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   5058\u001b[39m \u001b[43m        \u001b[49m\u001b[43mhf_quantizer\u001b[49m\u001b[43m=\u001b[49m\u001b[43mhf_quantizer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   5059\u001b[39m \u001b[43m        \u001b[49m\u001b[43mkeep_in_fp32_regex\u001b[49m\u001b[43m=\u001b[49m\u001b[43mkeep_in_fp32_regex\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   5060\u001b[39m \u001b[43m        \u001b[49m\u001b[43mdevice_mesh\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdevice_mesh\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   5061\u001b[39m \u001b[43m        \u001b[49m\u001b[43mkey_mapping\u001b[49m\u001b[43m=\u001b[49m\u001b[43mkey_mapping\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   5062\u001b[39m \u001b[43m        \u001b[49m\u001b[43mweights_only\u001b[49m\u001b[43m=\u001b[49m\u001b[43mweights_only\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   5063\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   5064\u001b[39m \u001b[38;5;66;03m# make sure token embedding weights are still tied if needed\u001b[39;00m\n\u001b[32m   5065\u001b[39m model.tie_weights()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/workspace/action-grounding/venv/lib/python3.11/site-packages/transformers/modeling_utils.py:5468\u001b[39m, in \u001b[36mPreTrainedModel._load_pretrained_model\u001b[39m\u001b[34m(cls, model, state_dict, checkpoint_files, pretrained_model_name_or_path, ignore_mismatched_sizes, sharded_metadata, device_map, disk_offload_folder, dtype, hf_quantizer, keep_in_fp32_regex, device_mesh, key_mapping, weights_only)\u001b[39m\n\u001b[32m   5465\u001b[39m         args_list = logging.tqdm(args_list, desc=\u001b[33m\"\u001b[39m\u001b[33mLoading checkpoint shards\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m   5467\u001b[39m     \u001b[38;5;28;01mfor\u001b[39;00m args \u001b[38;5;129;01min\u001b[39;00m args_list:\n\u001b[32m-> \u001b[39m\u001b[32m5468\u001b[39m         _error_msgs, disk_offload_index = \u001b[43mload_shard_file\u001b[49m\u001b[43m(\u001b[49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   5469\u001b[39m         error_msgs += _error_msgs\n\u001b[32m   5471\u001b[39m \u001b[38;5;66;03m# Save offloaded index if needed\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/workspace/action-grounding/venv/lib/python3.11/site-packages/transformers/modeling_utils.py:843\u001b[39m, in \u001b[36mload_shard_file\u001b[39m\u001b[34m(args)\u001b[39m\n\u001b[32m    841\u001b[39m \u001b[38;5;66;03m# Skip it with fsdp on ranks other than 0\u001b[39;00m\n\u001b[32m    842\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (is_fsdp_enabled() \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_local_dist_rank_0() \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_quantized):\n\u001b[32m--> \u001b[39m\u001b[32m843\u001b[39m     disk_offload_index = \u001b[43m_load_state_dict_into_meta_model\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    844\u001b[39m \u001b[43m        \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    845\u001b[39m \u001b[43m        \u001b[49m\u001b[43mstate_dict\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    846\u001b[39m \u001b[43m        \u001b[49m\u001b[43mshard_file\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    847\u001b[39m \u001b[43m        \u001b[49m\u001b[43mreverse_key_renaming_mapping\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    848\u001b[39m \u001b[43m        \u001b[49m\u001b[43mdevice_map\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdevice_map\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    849\u001b[39m \u001b[43m        \u001b[49m\u001b[43mdisk_offload_folder\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdisk_offload_folder\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    850\u001b[39m \u001b[43m        \u001b[49m\u001b[43mdisk_offload_index\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdisk_offload_index\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    851\u001b[39m \u001b[43m        \u001b[49m\u001b[43mhf_quantizer\u001b[49m\u001b[43m=\u001b[49m\u001b[43mhf_quantizer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    852\u001b[39m \u001b[43m        \u001b[49m\u001b[43mkeep_in_fp32_regex\u001b[49m\u001b[43m=\u001b[49m\u001b[43mkeep_in_fp32_regex\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    853\u001b[39m \u001b[43m        \u001b[49m\u001b[43mdevice_mesh\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdevice_mesh\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    854\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    856\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m error_msgs, disk_offload_index\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/workspace/action-grounding/venv/lib/python3.11/site-packages/torch/utils/_contextlib.py:120\u001b[39m, in \u001b[36mcontext_decorator.<locals>.decorate_context\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    117\u001b[39m \u001b[38;5;129m@functools\u001b[39m.wraps(func)\n\u001b[32m    118\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mdecorate_context\u001b[39m(*args, **kwargs):\n\u001b[32m    119\u001b[39m     \u001b[38;5;28;01mwith\u001b[39;00m ctx_factory():\n\u001b[32m--> \u001b[39m\u001b[32m120\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/workspace/action-grounding/venv/lib/python3.11/site-packages/transformers/modeling_utils.py:774\u001b[39m, in \u001b[36m_load_state_dict_into_meta_model\u001b[39m\u001b[34m(model, state_dict, shard_file, reverse_renaming_mapping, device_map, disk_offload_folder, disk_offload_index, hf_quantizer, keep_in_fp32_regex, device_mesh)\u001b[39m\n\u001b[32m    770\u001b[39m     _load_parameter_into_model(model, param_name, param.to(param_device))\n\u001b[32m    772\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    773\u001b[39m     \u001b[38;5;66;03m# TODO naming is stupid it loads it as well\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m774\u001b[39m     \u001b[43mhf_quantizer\u001b[49m\u001b[43m.\u001b[49m\u001b[43mcreate_quantized_param\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mparam\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mparam_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mparam_device\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    776\u001b[39m     \u001b[38;5;66;03m# For quantized modules with FSDP/DeepSpeed Stage 3, we need to quantize the parameter on the GPU\u001b[39;00m\n\u001b[32m    777\u001b[39m     \u001b[38;5;66;03m# and then cast it to CPU to avoid excessive memory usage on each GPU\u001b[39;00m\n\u001b[32m    778\u001b[39m     \u001b[38;5;66;03m# in comparison to the sharded model across GPUs.\u001b[39;00m\n\u001b[32m    779\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m is_fsdp_enabled() \u001b[38;5;129;01mor\u001b[39;00m is_deepspeed_zero3_enabled():\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/workspace/action-grounding/venv/lib/python3.11/site-packages/transformers/quantizers/quantizer_bnb_8bit.py:212\u001b[39m, in \u001b[36mBnb8BitHfQuantizer.create_quantized_param\u001b[39m\u001b[34m(self, model, param_value, param_name, target_device, **kwargs)\u001b[39m\n\u001b[32m    210\u001b[39m \u001b[38;5;66;03m# Need to pop SCB and reset it because of bnb internals that modifies its value when switching devices ...\u001b[39;00m\n\u001b[32m    211\u001b[39m SCB = kwargs.pop(\u001b[33m\"\u001b[39m\u001b[33mSCB\u001b[39m\u001b[33m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[32m--> \u001b[39m\u001b[32m212\u001b[39m new_value = \u001b[43mbnb\u001b[49m\u001b[43m.\u001b[49m\u001b[43mnn\u001b[49m\u001b[43m.\u001b[49m\u001b[43mInt8Params\u001b[49m\u001b[43m(\u001b[49m\u001b[43mparam_value\u001b[49m\u001b[43m.\u001b[49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mcpu\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrequires_grad\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtarget_device\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    213\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m SCB \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    214\u001b[39m     \u001b[38;5;28msetattr\u001b[39m(new_value, \u001b[33m\"\u001b[39m\u001b[33mSCB\u001b[39m\u001b[33m\"\u001b[39m, SCB)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/workspace/action-grounding/venv/lib/python3.11/site-packages/bitsandbytes/nn/modules.py:710\u001b[39m, in \u001b[36mInt8Params.to\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m    705\u001b[39m is_quantized = \u001b[38;5;28mself\u001b[39m.data.dtype == torch.int8\n\u001b[32m    707\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_quantized \u001b[38;5;129;01mand\u001b[39;00m device \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m device.type != \u001b[33m\"\u001b[39m\u001b[33mmeta\u001b[39m\u001b[33m\"\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m.data.device.type == \u001b[33m\"\u001b[39m\u001b[33mcpu\u001b[39m\u001b[33m\"\u001b[39m:\n\u001b[32m    708\u001b[39m     \u001b[38;5;66;03m# We're moving from a CPU device to a non-meta device.\u001b[39;00m\n\u001b[32m    709\u001b[39m     \u001b[38;5;66;03m# In this circumstance, we want to quantize if we haven't already.\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m710\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_quantize\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    712\u001b[39m \u001b[38;5;66;03m# Create a new parameter on the target device.\u001b[39;00m\n\u001b[32m    713\u001b[39m new_param = Int8Params(\n\u001b[32m    714\u001b[39m     \u001b[38;5;28msuper\u001b[39m().to(device=device, dtype=dtype, non_blocking=non_blocking),\n\u001b[32m    715\u001b[39m     requires_grad=\u001b[38;5;28mself\u001b[39m.requires_grad,\n\u001b[32m    716\u001b[39m     has_fp16_weights=\u001b[38;5;28mself\u001b[39m.has_fp16_weights,\n\u001b[32m    717\u001b[39m )\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/workspace/action-grounding/venv/lib/python3.11/site-packages/bitsandbytes/nn/modules.py:660\u001b[39m, in \u001b[36mInt8Params._quantize\u001b[39m\u001b[34m(self, device)\u001b[39m\n\u001b[32m    658\u001b[39m \u001b[38;5;66;03m# We quantize the weight and store in 8bit row-major\u001b[39;00m\n\u001b[32m    659\u001b[39m B = \u001b[38;5;28mself\u001b[39m.data.contiguous().to(device=device, dtype=torch.float16)\n\u001b[32m--> \u001b[39m\u001b[32m660\u001b[39m CB, SCB, _ = \u001b[43mbnb\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfunctional\u001b[49m\u001b[43m.\u001b[49m\u001b[43mint8_vectorwise_quant\u001b[49m\u001b[43m(\u001b[49m\u001b[43mB\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    661\u001b[39m \u001b[38;5;28mself\u001b[39m.data = CB\n\u001b[32m    662\u001b[39m \u001b[38;5;28mself\u001b[39m.CB = CB\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/workspace/action-grounding/venv/lib/python3.11/site-packages/bitsandbytes/functional.py:1962\u001b[39m, in \u001b[36mint8_vectorwise_quant\u001b[39m\u001b[34m(A, threshold)\u001b[39m\n\u001b[32m   1944\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mint8_vectorwise_quant\u001b[39m(A: torch.Tensor, threshold=\u001b[32m0.0\u001b[39m):\n\u001b[32m   1945\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"Quantizes a tensor with dtype `torch.float16` to `torch.int8` in accordance to the `LLM.int8()` algorithm.\u001b[39;00m\n\u001b[32m   1946\u001b[39m \n\u001b[32m   1947\u001b[39m \u001b[33;03m    For more information, see the [LLM.int8() paper](https://arxiv.org/abs/2208.07339).\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m   1960\u001b[39m \u001b[33;03m        - `torch.Tensor` with dtype `torch.int32`, *optional*: A list of column indices which contain outlier features.\u001b[39;00m\n\u001b[32m   1961\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m1962\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtorch\u001b[49m\u001b[43m.\u001b[49m\u001b[43mops\u001b[49m\u001b[43m.\u001b[49m\u001b[43mbitsandbytes\u001b[49m\u001b[43m.\u001b[49m\u001b[43mint8_vectorwise_quant\u001b[49m\u001b[43m.\u001b[49m\u001b[43mdefault\u001b[49m\u001b[43m(\u001b[49m\u001b[43mA\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mthreshold\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/workspace/action-grounding/venv/lib/python3.11/site-packages/torch/_ops.py:841\u001b[39m, in \u001b[36mOpOverload.__call__\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m    840\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m__call__\u001b[39m(\u001b[38;5;28mself\u001b[39m, /, *args: _P.args, **kwargs: _P.kwargs) -> _T:\n\u001b[32m--> \u001b[39m\u001b[32m841\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_op\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/workspace/action-grounding/venv/lib/python3.11/site-packages/torch/_compile.py:53\u001b[39m, in \u001b[36m_disable_dynamo.<locals>.inner\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m     50\u001b[39m     disable_fn = torch._dynamo.disable(fn, recursive, wrapping=\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[32m     51\u001b[39m     fn.__dynamo_disable = disable_fn  \u001b[38;5;66;03m# type: ignore[attr-defined]\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m53\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mdisable_fn\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/workspace/action-grounding/venv/lib/python3.11/site-packages/torch/_dynamo/eval_frame.py:1044\u001b[39m, in \u001b[36mDisableContext.__call__.<locals>._fn\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m   1042\u001b[39m _maybe_set_eval_frame(_callback_from_stance(\u001b[38;5;28mself\u001b[39m.callback))\n\u001b[32m   1043\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1044\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1045\u001b[39m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[32m   1046\u001b[39m     set_eval_frame(\u001b[38;5;28;01mNone\u001b[39;00m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/workspace/action-grounding/venv/lib/python3.11/site-packages/torch/library.py:732\u001b[39m, in \u001b[36m_impl.<locals>.register_.<locals>.func_no_dynamo\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    730\u001b[39m \u001b[38;5;129m@torch\u001b[39m._disable_dynamo\n\u001b[32m    731\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mfunc_no_dynamo\u001b[39m(*args, **kwargs):\n\u001b[32m--> \u001b[39m\u001b[32m732\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/workspace/action-grounding/venv/lib/python3.11/site-packages/bitsandbytes/backends/cuda/ops.py:136\u001b[39m, in \u001b[36m_\u001b[39m\u001b[34m(A, threshold)\u001b[39m\n\u001b[32m    133\u001b[39m cols = A.shape[-\u001b[32m1\u001b[39m]\n\u001b[32m    135\u001b[39m row_stats = torch.empty(rows, device=A.device, dtype=torch.float32)\n\u001b[32m--> \u001b[39m\u001b[32m136\u001b[39m out_row = \u001b[43mtorch\u001b[49m\u001b[43m.\u001b[49m\u001b[43mempty\u001b[49m\u001b[43m(\u001b[49m\u001b[43mA\u001b[49m\u001b[43m.\u001b[49m\u001b[43mshape\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m=\u001b[49m\u001b[43mA\u001b[49m\u001b[43m.\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtorch\u001b[49m\u001b[43m.\u001b[49m\u001b[43mint8\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    138\u001b[39m outlier_cols = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m    140\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m threshold > \u001b[32m0.0\u001b[39m:\n\u001b[32m    141\u001b[39m     \u001b[38;5;66;03m# TODO we could improve perf of this\u001b[39;00m\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "# Extract activations\n",
    "dataset = extract_activations_batch(\n",
    "    episodes=episodes,\n",
    "    positions=config.extraction.positions,\n",
    "    layers=config.extraction.layers,\n",
    "    model_id=config.model.id,\n",
    "    save_path=config.data.processed_dir / \"activations.parquet\",\n",
    "    verbose=True,\n",
    ")\n",
    "\n",
    "print(f\"\\nExtracted {len(dataset)} activation samples\")\n",
    "print(f\"Activation shape: {dataset.activations.shape}\")\n",
    "print(f\"Hidden size: {dataset.hidden_size}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load activations (if already extracted)\n",
    "# dataset = load_activations(config.data.processed_dir / \"activations.parquet\")\n",
    "# print(f\"Loaded {len(dataset)} activation samples\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Summary\n",
    "summary = dataset.summary()\n",
    "print(\"\\nDataset summary:\")\n",
    "print(f\"  Samples: {summary['n_samples']}\")\n",
    "print(f\"  Positions: {summary['positions']}\")\n",
    "print(f\"  Layers: {summary['layers']}\")\n",
    "print(f\"  Categories: {summary['categories']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Train Probes\n",
    "\n",
    "Train two probes:\n",
    "- **Reality probe:** Predicts `tool_used` (ground truth)\n",
    "- **Narrative probe:** Predicts `claims_action` (model's claim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter to single position and layer for initial probe\n",
    "# Use mid_response at layer 16 (middle layer)\n",
    "probe_dataset = dataset.filter_by_position(\"mid_response\").filter_by_layer(16)\n",
    "\n",
    "print(f\"Probe training dataset: {len(probe_dataset)} samples\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train reality probe\n",
    "reality_probe, reality_train_metrics, reality_test_metrics = train_and_evaluate(\n",
    "    probe_dataset,\n",
    "    label_type=\"reality\",\n",
    "    test_size=0.2,\n",
    "    random_state=42,\n",
    ")\n",
    "\n",
    "print(\"\\nReality Probe (predicts tool_used):\")\n",
    "print(\"Train Metrics:\")\n",
    "print(reality_train_metrics)\n",
    "print(\"\\nTest Metrics:\")\n",
    "print(reality_test_metrics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train narrative probe\n",
    "narrative_probe, narrative_train_metrics, narrative_test_metrics = train_and_evaluate(\n",
    "    probe_dataset,\n",
    "    label_type=\"narrative\",\n",
    "    test_size=0.2,\n",
    "    random_state=42,\n",
    ")\n",
    "\n",
    "print(\"\\nNarrative Probe (predicts claims_action):\")\n",
    "print(\"Train Metrics:\")\n",
    "print(narrative_train_metrics)\n",
    "print(\"\\nTest Metrics:\")\n",
    "print(narrative_test_metrics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save probes\n",
    "save_probe(reality_probe, config.data.processed_dir / \"reality_probe.pkl\")\n",
    "save_probe(narrative_probe, config.data.processed_dir / \"narrative_probe.pkl\")\n",
    "\n",
    "print(\"Probes saved.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. CRITICAL: Position Analysis\n",
    "\n",
    "**Key test:** Does the probe work at `first_assistant` (before any tool tokens)?\n",
    "\n",
    "If yes → probe detects action-grounding, not syntax  \n",
    "If no → probe might just be detecting `<<CALL` tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train probes at each position (using layer 16)\n",
    "position_accuracies = {}\n",
    "\n",
    "for position in config.extraction.positions:\n",
    "    print(f\"\\nTraining probe at position: {position}\")\n",
    "    \n",
    "    pos_dataset = dataset.filter_by_position(position).filter_by_layer(16)\n",
    "    \n",
    "    if len(pos_dataset) == 0:\n",
    "        print(f\"  No samples found for {position}\")\n",
    "        continue\n",
    "    \n",
    "    probe, _, test_metrics = train_and_evaluate(\n",
    "        pos_dataset,\n",
    "        label_type=\"reality\",\n",
    "        random_state=42,\n",
    "    )\n",
    "    \n",
    "    position_accuracies[position] = test_metrics.accuracy\n",
    "    print(f\"  Test accuracy: {test_metrics.accuracy:.1%}\")\n",
    "\n",
    "print(\"\\nPosition accuracies:\")\n",
    "for pos, acc in position_accuracies.items():\n",
    "    status = \"✓ PASS\" if acc > 0.80 else \"✗ FAIL\"\n",
    "    print(f\"  {pos}: {acc:.1%} {status}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize position analysis\n",
    "fig = plot_position_accuracy(\n",
    "    position_accuracies,\n",
    "    title=\"Reality Probe Accuracy by Token Position\",\n",
    "    save_path=config.figures_dir / \"figure2_position_accuracy\",\n",
    ")\n",
    "\n",
    "plt.show()\n",
    "\n",
    "# Check critical result\n",
    "first_assistant_acc = position_accuracies.get('first_assistant', 0)\n",
    "if first_assistant_acc > 0.80:\n",
    "    print(f\"\\n✓ CRITICAL RESULT: first_assistant accuracy = {first_assistant_acc:.1%} > 80%\")\n",
    "    print(\"  → Probe detects action-grounding, not just syntax!\")\n",
    "else:\n",
    "    print(f\"\\n✗ WARNING: first_assistant accuracy = {first_assistant_acc:.1%} < 80%\")\n",
    "    print(\"  → May be detecting syntax, not action-grounding\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Analyze Fake Action Cases\n",
    "\n",
    "**Critical test:** Does the probe correctly identify fake action episodes?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze probe on fake action cases\n",
    "fake_analysis = analyze_probe_on_category(\n",
    "    reality_probe,\n",
    "    probe_dataset,\n",
    "    category=\"fake_action\",\n",
    "    label_type=\"reality\",\n",
    ")\n",
    "\n",
    "print(f\"\\nFake Action Analysis:\")\n",
    "print(f\"  N samples: {fake_analysis['n_samples']}\")\n",
    "print(f\"  Metrics:\")\n",
    "print(fake_analysis['metrics'])\n",
    "\n",
    "# Check alignment with ground truth\n",
    "fake_probs = fake_analysis['probabilities']\n",
    "fake_preds = fake_analysis['predictions']\n",
    "fake_labels = fake_analysis['true_labels']  # Should all be 0 (tool not used)\n",
    "\n",
    "# How many does probe correctly identify as \"tool not used\"?\n",
    "correct_on_fakes = np.mean(fake_preds == 0)\n",
    "mean_prob_tool_used = np.mean(fake_probs)\n",
    "\n",
    "print(f\"\\n**Probe on Fake Actions:**\")\n",
    "print(f\"  Correctly predicts 'no tool': {correct_on_fakes:.1%}\")\n",
    "print(f\"  Mean P(tool_used): {mean_prob_tool_used:.3f}\")\n",
    "\n",
    "if correct_on_fakes > 0.95:\n",
    "    print(\"  ✓ Probe aligns with reality, not narrative!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Histogram of P(tool_used) on fake vs true cases\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# Get predictions on true action cases for comparison\n",
    "true_analysis = analyze_probe_on_category(\n",
    "    reality_probe,\n",
    "    probe_dataset,\n",
    "    category=\"true_action\",\n",
    "    label_type=\"reality\",\n",
    ")\n",
    "\n",
    "# Plot fake action probabilities\n",
    "axes[0].hist(fake_probs, bins=20, alpha=0.7, color='red', edgecolor='black')\n",
    "axes[0].axvline(x=0.5, color='k', linestyle='--', linewidth=1)\n",
    "axes[0].set_xlabel('P(tool_used)')\n",
    "axes[0].set_ylabel('Count')\n",
    "axes[0].set_title(f'Fake Actions (should cluster near 0)\\nMean = {mean_prob_tool_used:.3f}')\n",
    "axes[0].set_xlim(0, 1)\n",
    "\n",
    "# Plot true action probabilities\n",
    "true_probs = true_analysis['probabilities']\n",
    "mean_prob_true = np.mean(true_probs)\n",
    "axes[1].hist(true_probs, bins=20, alpha=0.7, color='green', edgecolor='black')\n",
    "axes[1].axvline(x=0.5, color='k', linestyle='--', linewidth=1)\n",
    "axes[1].set_xlabel('P(tool_used)')\n",
    "axes[1].set_ylabel('Count')\n",
    "axes[1].set_title(f'True Actions (should cluster near 1)\\nMean = {mean_prob_true:.3f}')\n",
    "axes[1].set_xlim(0, 1)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(config.figures_dir / \"figure3_fake_vs_true_probs.png\", dpi=300, bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Layer Analysis\n",
    "\n",
    "Which layers encode action-grounding information?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train probes at each layer (using mid_response position)\n",
    "layer_accuracies = {}\n",
    "\n",
    "for layer in config.extraction.layers:\n",
    "    print(f\"\\nTraining probe at layer: {layer}\")\n",
    "    \n",
    "    layer_dataset = dataset.filter_by_position(\"mid_response\").filter_by_layer(layer)\n",
    "    \n",
    "    if len(layer_dataset) == 0:\n",
    "        print(f\"  No samples found for layer {layer}\")\n",
    "        continue\n",
    "    \n",
    "    probe, _, test_metrics = train_and_evaluate(\n",
    "        layer_dataset,\n",
    "        label_type=\"reality\",\n",
    "        random_state=42,\n",
    "    )\n",
    "    \n",
    "    layer_accuracies[layer] = test_metrics.accuracy\n",
    "    print(f\"  Test accuracy: {test_metrics.accuracy:.1%}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize layer analysis\n",
    "fig = plot_layer_analysis(\n",
    "    layer_accuracies,\n",
    "    title=\"Reality Probe Accuracy by Layer\",\n",
    "    save_path=config.figures_dir / \"figure5_layer_accuracy\",\n",
    ")\n",
    "\n",
    "plt.show()\n",
    "\n",
    "# Best layer\n",
    "best_layer = max(layer_accuracies.items(), key=lambda x: x[1])\n",
    "print(f\"\\nBest layer: {best_layer[0]} (accuracy: {best_layer[1]:.1%})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Probe Direction Analysis\n",
    "\n",
    "Are reality and narrative probes learning the same representation?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare probe directions\n",
    "from src.analysis.probes import compare_probes\n",
    "\n",
    "comparison = compare_probes(reality_probe, narrative_probe, normalize=True)\n",
    "\n",
    "print(f\"\\nReality vs Narrative Probe:\")\n",
    "print(f\"  Cosine similarity: {comparison['cosine_similarity']:.3f}\")\n",
    "print(f\"  L2 distance: {comparison['l2_distance']:.3f}\")\n",
    "\n",
    "if abs(comparison['cosine_similarity']) > 0.8:\n",
    "    print(\"  → Probes learn similar directions (aligned)\")\n",
    "elif abs(comparison['cosine_similarity']) < 0.3:\n",
    "    print(\"  → Probes learn different directions (independent representations)\")\n",
    "else:\n",
    "    print(\"  → Probes partially aligned\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Visualizations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get test data for reality probe\n",
    "train_dataset, test_dataset = probe_dataset.train_test_split(test_size=0.2, random_state=42)\n",
    "X_test, y_test = test_dataset.to_sklearn_format(\"reality\")\n",
    "\n",
    "# Confusion matrix\n",
    "y_pred = reality_probe.predict(X_test)\n",
    "fig = plot_confusion_matrix(\n",
    "    y_test,\n",
    "    y_pred,\n",
    "    labels=[\"No Tool\", \"Tool Used\"],\n",
    "    title=\"Reality Probe Confusion Matrix\",\n",
    "    save_path=config.figures_dir / \"reality_probe_confusion\",\n",
    ")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ROC curve\n",
    "y_proba = reality_probe.predict_proba(X_test)[:, 1]\n",
    "auc, fpr, tpr, thresholds = compute_roc_auc(y_test, y_proba)\n",
    "\n",
    "fig = plot_roc_curve(\n",
    "    fpr,\n",
    "    tpr,\n",
    "    auc,\n",
    "    title=\"Reality Probe ROC Curve\",\n",
    "    save_path=config.figures_dir / \"reality_probe_roc\",\n",
    ")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\" * 60)\n",
    "print(\"PHASE 2 RESULTS: MECHANISTIC PROBES\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "print(f\"\\nReality Probe Performance:\")\n",
    "print(f\"  Test Accuracy: {reality_test_metrics.accuracy:.1%}\")\n",
    "print(f\"  ROC-AUC: {reality_test_metrics.roc_auc:.3f}\")\n",
    "\n",
    "print(f\"\\nPosition Analysis:\")\n",
    "for pos, acc in position_accuracies.items():\n",
    "    print(f\"  {pos}: {acc:.1%}\")\n",
    "\n",
    "print(f\"\\nFake Action Analysis:\")\n",
    "print(f\"  Correct on fakes: {correct_on_fakes:.1%}\")\n",
    "print(f\"  Mean P(tool_used) on fakes: {mean_prob_tool_used:.3f}\")\n",
    "\n",
    "print(f\"\\nProbe Direction Comparison:\")\n",
    "print(f\"  Cosine similarity: {comparison['cosine_similarity']:.3f}\")\n",
    "\n",
    "print(\"\\n✓ Phase 2 complete: Linear probe can detect ground truth\")\n",
    "print(\"=\" * 60)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Next Steps\n",
    "\n",
    "→ **Notebook 03:** Test cross-tool generalization"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (interpret venv)",
   "language": "python",
   "name": "interpret"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
