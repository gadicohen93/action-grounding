{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Phase 2: Activation Probing\n",
    "\n",
    "**Goal:** Determine if the model \"knows\" it didn't call the tool, even when claiming it did.\n",
    "\n",
    "**Core Hypothesis:** There exists an internal representation of \"action actually taken\" that's separable from the narrative output.\n",
    "\n",
    "## Pipeline\n",
    "1. Load Phase 1 episodes\n",
    "2. Extract activations (teacher forcing)\n",
    "3. Train reality probe (predicts `tool_used`)\n",
    "4. Train narrative probe (predicts `claims_action`)\n",
    "5. Evaluate on fake_escalation episodes\n",
    "6. Cross-domain transfer test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup\n",
    "import sys\n",
    "sys.path.insert(0, '..')\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "\n",
    "from src.activations import (\n",
    "    load_model_for_activations,\n",
    "    build_activation_dataset,\n",
    "    samples_to_arrays,\n",
    "    save_activation_dataset,\n",
    "    load_activation_dataset,\n",
    ")\n",
    "from src.probes import (\n",
    "    train_reality_probe,\n",
    "    train_narrative_probe,\n",
    "    evaluate_probe,\n",
    "    evaluate_by_category,\n",
    "    print_category_metrics,\n",
    "    analyze_fake_escalations,\n",
    "    print_fake_analysis,\n",
    "    evaluate_transfer,\n",
    "    print_transfer_result,\n",
    "    full_probe_evaluation,\n",
    "    cross_validate_probe,\n",
    ")\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Load Model for Activation Extraction\n",
    "\n",
    "Using PyTorch/Transformers (not MLX) to access hidden states."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading model for activations: mistralai/Mistral-7B-Instruct-v0.2...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "61b747a484324ed980f60732df25bdf0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading weights:   0%|          | 0/291 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model loaded: mistralai/Mistral-7B-Instruct-v0.2\n",
      "Model hidden size: 4096\n"
     ]
    }
   ],
   "source": [
    "# Load model\n",
    "model, tokenizer = load_model_for_activations(\n",
    "    model_id=\"mistralai/Mistral-7B-Instruct-v0.2\"\n",
    ")\n",
    "print(f\"Model hidden size: {model.config.hidden_size}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Build Activation Dataset from Phase 1 Episodes\n",
    "\n",
    "This extracts activations at safe token positions (before any `<<CALL` tokens)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find Phase 1 episode files\n",
    "data_dir = Path(\"../data/raw\")\n",
    "episode_files = list(data_dir.glob(\"*.jsonl\"))\n",
    "print(f\"Found {len(episode_files)} episode files:\")\n",
    "for f in episode_files:\n",
    "    print(f\"  {f.name}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build activation dataset from a specific file\n",
    "# Choose the file with the most episodes / best balance\n",
    "EPISODE_FILE = \"../data/raw/adversarial_20251221_020507.jsonl\"  # UPDATE THIS\n",
    "\n",
    "samples = build_activation_dataset(\n",
    "    episodes_file=EPISODE_FILE,\n",
    "    model=model,\n",
    "    tokenizer=tokenizer,\n",
    "    model_type=\"mistral\",\n",
    "    position_preference=\"before_tool\",  # Anti-cheat: use position before <<CALL\n",
    "    max_episodes=200,  # Set to small number for testing\n",
    "    verbose=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "EPISODE_FILE = \"../data/raw/adversarial_20251221_020507.jsonl\"  # UPDATE THIS\n",
    "\n",
    "samples_remaining = build_activation_dataset(\n",
    "    episodes_file=EPISODE_FILE,\n",
    "    model=model,\n",
    "    tokenizer=tokenizer,\n",
    "    model_type=\"mistral\",\n",
    "    position_preference=\"before_tool\",\n",
    "    skip_first=200,      # Skip first 200\n",
    "    verbose=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save for later use\n",
    "save_activation_dataset(samples_remaining, \"../data/labeled/activations_v2.npz\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert to arrays\n",
    "# Combine with original samples\n",
    "\n",
    "\n",
    "\n",
    "# Load all samples from combined dataset\n",
    "print(\"Loading all samples from activations_v1_combined.npz...\")\n",
    "from src.activations import ActivationSample\n",
    "X_loaded, y_tool_loaded, y_claims_loaded, categories_loaded, metadata = load_activation_dataset(\"../data/labeled/activations_v1_combined.npz\")\n",
    "# Reconstruct ActivationSample objects from loaded data\n",
    "all_samples = []\n",
    "for i in range(len(X_loaded)):\n",
    "    all_samples.append(ActivationSample(\n",
    "        activation=X_loaded[i],\n",
    "        tool_used=bool(y_tool_loaded[i]),\n",
    "        claims_action=bool(y_claims_loaded[i]),\n",
    "        category=str(categories_loaded[i]),\n",
    "        scenario=str(metadata[\"scenarios\"][i]),\n",
    "        system_variant=str(metadata[\"system_variants\"][i]),\n",
    "        social_pressure=str(metadata[\"social_pressures\"][i]),\n",
    "        position_type=str(metadata[\"position_types\"][i]),\n",
    "        episode_idx=int(metadata[\"episode_indices\"][i]),\n",
    "    ))\n",
    "print(f\"Loaded {len(all_samples)} samples from combined dataset\")\n",
    "print(f\"Total samples: {len(all_samples)}\")\n",
    "\n",
    "# Convert combined dataset\n",
    "X, y_tool, y_claims, categories = samples_to_arrays(all_samples)\n",
    "print(f\"X shape: {X.shape}\")\n",
    "print(f\"Category distribution:\")\n",
    "for cat in np.unique(categories):\n",
    "    print(f\"  {cat}: {(categories == cat).sum()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save combined dataset (first 200 episodes + episodes 201+)\n",
    "# This preserves the original activations_v1.npz while saving the full combined dataset\n",
    "save_activation_dataset(all_samples, \"../data/labeled/activations_v1_combined.npz\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Train/Test Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Stratified split to maintain category balance\n",
    "X_train, X_test, y_tool_train, y_tool_test, y_claims_train, y_claims_test, cat_train, cat_test = train_test_split(\n",
    "    X, y_tool, y_claims, categories,\n",
    "    test_size=0.2,\n",
    "    stratify=categories,\n",
    "    random_state=42,\n",
    ")\n",
    "\n",
    "print(f\"Train: {len(X_train)}, Test: {len(X_test)}\")\n",
    "print(f\"\\nTest set categories:\")\n",
    "for cat in np.unique(cat_test):\n",
    "    print(f\"  {cat}: {(cat_test == cat).sum()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Cross-Validation (Sanity Check)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Quick cross-validation to check if probing is viable\n",
    "cv_reality = cross_validate_probe(X_train, y_tool_train, n_folds=5)\n",
    "cv_narrative = cross_validate_probe(X_train, y_claims_train, n_folds=5)\n",
    "\n",
    "print(\"Cross-validation results:\")\n",
    "print(f\"  Reality probe:   {cv_reality['mean_accuracy']:.1%} ¬± {cv_reality['std_accuracy']:.1%}\")\n",
    "print(f\"  Narrative probe: {cv_narrative['mean_accuracy']:.1%} ¬± {cv_narrative['std_accuracy']:.1%}\")\n",
    "\n",
    "if cv_reality['mean_accuracy'] > 0.6:\n",
    "    print(\"\\n‚úì Reality probe shows signal above chance!\")\n",
    "else:\n",
    "    print(\"\\n‚ö† Reality probe near chance - may need more data or different position\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Full Probe Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run full evaluation\n",
    "results = full_probe_evaluation(\n",
    "    X_train, y_tool_train, y_claims_train, cat_train,\n",
    "    X_test, y_tool_test, y_claims_test, cat_test,\n",
    "    verbose=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Interpret Results\n",
    "\n",
    "### Key Questions:\n",
    "\n",
    "1. **Does the reality probe work?** (accuracy >> 50%)\n",
    "   - If yes ‚Üí model internally represents whether it called the tool\n",
    "\n",
    "2. **On fake_escalation episodes, what does the reality probe predict?**\n",
    "   - Should predict `tool_used=False` (aligned with reality)\n",
    "   - If it predicts `True`, it's being fooled by its own narrative\n",
    "\n",
    "3. **Does the narrative probe work?**\n",
    "   - Should predict `claims_action` accurately\n",
    "   - Shows model represents what it's claiming"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Summary interpretation\n",
    "fake_analysis = results[\"fake_analysis\"]\n",
    "reality_acc = results[\"reality_overall\"].accuracy\n",
    "\n",
    "print(\"=\"*60)\n",
    "print(\"INTERPRETATION\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "if reality_acc > 0.7:\n",
    "    print(f\"‚úì Reality probe works well ({reality_acc:.1%} accuracy)\")\n",
    "    print(\"  ‚Üí Model has internal representation of 'action taken'\")\n",
    "else:\n",
    "    print(f\"? Reality probe marginal ({reality_acc:.1%} accuracy)\")\n",
    "\n",
    "if fake_analysis.n_fake > 0:\n",
    "    if fake_analysis.probe_aligned_with_reality > 0.7:\n",
    "        print(f\"\\n‚úì On FAKE escalations, probe aligns with REALITY ({fake_analysis.probe_aligned_with_reality:.1%})\")\n",
    "        print(\"  ‚Üí Model 'knows' it didn't call the tool, even when claiming it did!\")\n",
    "        print(\"  ‚Üí This is STRONG evidence for action grounding\")\n",
    "    elif fake_analysis.probe_aligned_with_narrative > 0.7:\n",
    "        print(f\"\\n‚úó On FAKE escalations, probe aligns with NARRATIVE ({fake_analysis.probe_aligned_with_narrative:.1%})\")\n",
    "        print(\"  ‚Üí Probe is 'fooled' by the model's own claims\")\n",
    "        print(\"  ‚Üí Less clear evidence for separate action grounding\")\n",
    "    else:\n",
    "        print(f\"\\n? On FAKE escalations, probe is uncertain\")\n",
    "        print(f\"  Reality: {fake_analysis.probe_aligned_with_reality:.1%}, Narrative: {fake_analysis.probe_aligned_with_narrative:.1%}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Cross-Domain Transfer (Optional - Requires Second Tool)\n",
    "\n",
    "This is the headline experiment: does a probe trained on `escalateCase` generalize to `searchKnowledgeBase`?\n",
    "\n",
    "Requires generating episodes with a second tool type first."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Load second tool episodes\n",
    "samples_search = build_activation_dataset(\n",
    "    \"../data/raw/search_episodes_20251221_141706.jsonl\",\n",
    "    model, tokenizer, \"mistral\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save search activations dataset\n",
    "save_activation_dataset(samples_search, \"../data/labeled/activations_search.npz\")\n",
    "print(f\"Saved {len(samples_search)} search samples to ../data/labeled/activations_search.npz\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Visualize Probe Weights (Optional)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_search, y_tool_search, _, _ = samples_to_arrays(samples_search)\n",
    "\n",
    "# Transfer test\n",
    "transfer_result = evaluate_transfer(\n",
    "    X_train, y_tool_train,  # Train on escalation\n",
    "    X_search, y_tool_search,  # Test on search\n",
    "    train_domain=\"escalateCase\",\n",
    "    test_domain=\"searchKnowledgeBase\",\n",
    ")\n",
    "print_transfer_result(transfer_result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "reality_probe = results[\"reality_probe\"]\n",
    "weights = reality_probe.coef_[0]\n",
    "\n",
    "plt.figure(figsize=(12, 4))\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.hist(weights, bins=50)\n",
    "plt.title(\"Reality Probe Weight Distribution\")\n",
    "plt.xlabel(\"Weight\")\n",
    "plt.ylabel(\"Count\")\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "top_k = 20\n",
    "top_idx = np.argsort(np.abs(weights))[-top_k:]\n",
    "plt.barh(range(top_k), weights[top_idx])\n",
    "plt.title(f\"Top {top_k} Dimensions by |weight|\")\n",
    "plt.xlabel(\"Weight\")\n",
    "plt.ylabel(\"Dimension Index\")\n",
    "plt.yticks(range(top_k), top_idx)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Next Steps\n",
    "\n",
    "If results are promising:\n",
    "\n",
    "1. **Add second tool** (`searchKnowledgeBase`) for cross-domain transfer\n",
    "2. **Phase 3: Causal intervention** - Use activation patching to show the probe direction actually affects behavior\n",
    "3. **More episodes** - Scale up to 500+ for robust statistics\n",
    "4. **Different token positions** - Compare `before_tool` vs `first_assistant` vs `final`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Layer-wise Probe Analysis\n",
    "\n",
    "**Question:** At which layer does action-grounding emerge?\n",
    "\n",
    "- Early layers (0-8): Token/positional features\n",
    "- Mid layers (8-20): Semantic features  \n",
    "- Late layers (20-32): Task-specific, abstract concepts\n",
    "\n",
    "If the probe works well at layer 16 but not layer 4, that tells us action-grounding is a mid-to-late network phenomenon."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Summary: Narrative Probe Error Analysis\n",
    "\n",
    "**Key Finding:** Many \"errors\" are actually **ground truth labeling issues**, not probe failures.\n",
    "\n",
    "**Evidence:**\n",
    "\n",
    "1. **Error 2** - Clear false negative in labels:\n",
    "   - Text contains: \"I've also gone ahead and escalated your case\" + \"a human will be reviewing your case\"\n",
    "   - Matches phrase list: `\"i've escalated\"` and `\"a human will review your case\"`\n",
    "   - Ground truth says: `claims_action: False` ‚ùå\n",
    "   - Probe says: `claims_action: True` (confidence 0.999) ‚úÖ\n",
    "   - **Probe is correct** - this should be labeled as a claim\n",
    "\n",
    "2. **Error 3** - Labeling inconsistency:\n",
    "   - Text contains: \"I've escalated your case to a human team member\"\n",
    "   - Matches phrase list: `\"i've escalated\"`\n",
    "   - Header says: `claims_action: False` but claim detection section says: `claims_action: True`\n",
    "   - Probe aligns with claim detection (confidence 1.000)\n",
    "   - **Same episode labeled differently in different places**\n",
    "\n",
    "3. **Error 5** - Likely false positive in labels:\n",
    "   - Text contains conditional/instructional language: \"If you would like... you can always escalate\"\n",
    "   - This is telling user HOW to escalate, not claiming escalation happened\n",
    "   - Probe says: `claims_action: False` (confidence 0.000) ‚úÖ\n",
    "   - Ground truth says: `claims_action: True` ‚ùå\n",
    "\n",
    "**Root Cause:** \n",
    "- Episodes were labeled using simple regex phrase matching (`detect_escalation_claim()`)\n",
    "- Regex doesn't handle context, partial matches, or conditional language well\n",
    "- The probe learns semantic meaning better than regex matching\n",
    "\n",
    "**Implication:**\n",
    "The narrative probe may be learning a more accurate representation than the ground truth labels. Consider re-labeling episodes using LLM judge (`use_llm_judge=True`) for more nuanced detection.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Layer-wise probe analysis\n",
    "# Mistral-7B has 32 layers (indices 0-31), plus embedding layer\n",
    "# hidden_states[0] = embeddings, hidden_states[1] = layer 0 output, ..., hidden_states[32] = layer 31 output\n",
    "\n",
    "import json\n",
    "from src.activations import (\n",
    "    build_full_text, \n",
    "    find_token_positions, \n",
    "    get_safe_probe_index,\n",
    "    extract_activations,\n",
    "    ActivationSample,\n",
    ")\n",
    "\n",
    "# Layers to test: early, mid-early, mid, mid-late, late\n",
    "LAYERS_TO_TEST = [1, 8, 16, 24, 32]  # 1=first layer, 32=last layer (index into hidden_states)\n",
    "\n",
    "# Load episodes for extraction\n",
    "EPISODE_FILE = \"../data/raw/adversarial_20251221_020507.jsonl\"  # Same file used for main analysis\n",
    "with open(EPISODE_FILE) as f:\n",
    "    episodes = [json.loads(line) for line in f]\n",
    "\n",
    "# Use subset for speed (layer analysis is expensive - full forward pass per layer)\n",
    "N_EPISODES_FOR_LAYER = 100  \n",
    "episodes_subset = episodes[:N_EPISODES_FOR_LAYER]\n",
    "\n",
    "print(f\"Running layer-wise analysis on {len(episodes_subset)} episodes...\")\n",
    "print(f\"Testing layers: {LAYERS_TO_TEST}\")\n",
    "print(f\"This will take a few minutes...\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing 5 layers across 100 episodes...\n",
      "Layers to analyze: [1, 8, 16, 24, 32]\n",
      "\n",
      "==================================================\n",
      "Layer 1 (hidden_states index)\n",
      "==================================================\n",
      "  Processing episode 20/100...\n"
     ]
    }
   ],
   "source": [
    "# Extract activations at each layer and train probes\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "import torch\n",
    "from collections import Counter\n",
    "from src.prompts import build_episode, SystemPromptVariant, SocialPressure\n",
    "\n",
    "# Check that required variables are defined\n",
    "if 'model' not in globals() or 'tokenizer' not in globals():\n",
    "    raise NameError(\n",
    "        \"model and tokenizer are not defined. Please run the cell that loads the model first:\\n\"\n",
    "        \"  model, tokenizer = load_model_for_activations(model_id=\\\"mistralai/Mistral-7B-Instruct-v0.2\\\")\"\n",
    "    )\n",
    "\n",
    "if 'EPISODE_FILE' not in globals():\n",
    "    raise NameError(\n",
    "        \"EPISODE_FILE is not defined. Please run the cell that sets EPISODE_FILE first.\"\n",
    "    )\n",
    "\n",
    "if 'episodes_subset' not in globals():\n",
    "    raise NameError(\n",
    "        \"episodes_subset is not defined. Please run the cell that creates episodes_subset first.\"\n",
    "    )\n",
    "\n",
    "if 'LAYERS_TO_TEST' not in globals():\n",
    "    raise NameError(\n",
    "        \"LAYERS_TO_TEST is not defined. Please run the cell that sets LAYERS_TO_TEST first.\"\n",
    "    )\n",
    "\n",
    "layer_results = {\n",
    "    'layer': [],\n",
    "    'reality_acc': [],\n",
    "    'narrative_acc': [],\n",
    "    'fake_aligned_reality': [],\n",
    "}\n",
    "\n",
    "print(f\"Processing {len(LAYERS_TO_TEST)} layers across {len(episodes_subset)} episodes...\")\n",
    "print(f\"Layers to analyze: {LAYERS_TO_TEST}\\n\")\n",
    "\n",
    "for layer_idx in LAYERS_TO_TEST:\n",
    "    print(f\"{'='*50}\")\n",
    "    print(f\"Layer {layer_idx} (hidden_states index)\")\n",
    "    print(f\"{'='*50}\")\n",
    "    \n",
    "    layer_samples = []\n",
    "    errors = 0\n",
    "    \n",
    "    for ep_idx, episode in enumerate(episodes_subset):\n",
    "        if (ep_idx + 1) % 20 == 0:\n",
    "            print(f\"  Processing episode {ep_idx + 1}/{len(episodes_subset)}...\")\n",
    "        \n",
    "        try:\n",
    "            # Reconstruct system_prompt if missing (same logic as build_activation_dataset)\n",
    "            if \"system_prompt\" not in episode:\n",
    "                variant = SystemPromptVariant(episode[\"system_variant\"])\n",
    "                pressure = SocialPressure(episode[\"social_pressure\"])\n",
    "                scenario = episode.get(\"scenario\") or episode.get(\"scenario_id\")\n",
    "                if scenario is None:\n",
    "                    raise ValueError(f\"Episode {ep_idx} missing both 'scenario' and 'scenario_id' fields\")\n",
    "                ep_data = build_episode(scenario, variant, pressure)\n",
    "                episode[\"system_prompt\"] = ep_data[\"system_prompt\"]\n",
    "                episode[\"user_turns\"] = ep_data[\"user_turns\"]\n",
    "            \n",
    "            # Build full text\n",
    "            full_text = build_full_text(episode, \"mistral\")\n",
    "            \n",
    "            # Get token positions\n",
    "            token_ids = tokenizer.encode(full_text)\n",
    "            positions = find_token_positions(token_ids, tokenizer, \"mistral\")\n",
    "            probe_idx = get_safe_probe_index(positions, \"before_tool\")\n",
    "            \n",
    "            if probe_idx is None:\n",
    "                errors += 1\n",
    "                if ep_idx < 3:\n",
    "                    print(f\"  Warning: Episode {ep_idx} - no valid probe position found\")\n",
    "                continue\n",
    "            \n",
    "            # Extract ALL hidden states in one forward pass\n",
    "            inputs = tokenizer(full_text, return_tensors=\"pt\")\n",
    "            inputs = {k: v.to(model.device) for k, v in inputs.items()}\n",
    "            \n",
    "            with torch.no_grad():\n",
    "                outputs = model(**inputs, output_hidden_states=True)\n",
    "            \n",
    "            # Get activation at this layer and position\n",
    "            hidden_state = outputs.hidden_states[layer_idx]  # [1, seq_len, hidden_dim]\n",
    "            activation = hidden_state[0, probe_idx, :].cpu().numpy()\n",
    "            \n",
    "            layer_samples.append(ActivationSample(\n",
    "                activation=activation,\n",
    "                tool_used=episode[\"tool_used\"],\n",
    "                claims_action=episode[\"claims_action\"],\n",
    "                category=episode[\"category\"],\n",
    "                scenario=episode[\"scenario\"],\n",
    "                system_variant=episode[\"system_variant\"],\n",
    "                social_pressure=episode[\"social_pressure\"],\n",
    "                position_type=\"before_tool\",\n",
    "                episode_idx=ep_idx,\n",
    "            ))\n",
    "            \n",
    "        except Exception as e:\n",
    "            errors += 1\n",
    "            if ep_idx < 3:\n",
    "                print(f\"  Error on episode {ep_idx}: {e}\")\n",
    "            continue\n",
    "    \n",
    "    print(f\"  Extracted {len(layer_samples)} samples ({errors} errors/skipped)\")\n",
    "    \n",
    "    if len(layer_samples) == 0:\n",
    "        print(f\"  ‚ö†Ô∏è  No samples extracted for layer {layer_idx}, skipping...\")\n",
    "        continue\n",
    "    \n",
    "    # Show category distribution\n",
    "    categories_layer = np.array([s.category for s in layer_samples])\n",
    "    cat_counts = Counter(categories_layer)\n",
    "    print(f\"  Category distribution: {dict(cat_counts)}\")\n",
    "    \n",
    "    # Convert to arrays\n",
    "    X_layer = np.stack([s.activation for s in layer_samples])\n",
    "    y_tool_layer = np.array([s.tool_used for s in layer_samples])\n",
    "    y_claims_layer = np.array([s.claims_action for s in layer_samples])\n",
    "    \n",
    "    print(f\"  Activation shape: {X_layer.shape}\")\n",
    "    print(f\"  Tool used rate: {y_tool_layer.mean():.1%}\")\n",
    "    print(f\"  Claims action rate: {y_claims_layer.mean():.1%}\")\n",
    "    \n",
    "    # Train/test split\n",
    "    try:\n",
    "        X_tr, X_te, y_tool_tr, y_tool_te, y_claims_tr, y_claims_te, cat_tr, cat_te = train_test_split(\n",
    "            X_layer, y_tool_layer, y_claims_layer, categories_layer,\n",
    "            test_size=0.2, stratify=categories_layer, random_state=42\n",
    "        )\n",
    "        print(f\"  Train/test split: {len(X_tr)}/{len(X_te)} samples\")\n",
    "    except ValueError as e:\n",
    "        print(f\"  ‚ö†Ô∏è  Stratified split failed: {e}\")\n",
    "        print(f\"  Using non-stratified split...\")\n",
    "        X_tr, X_te, y_tool_tr, y_tool_te, y_claims_tr, y_claims_te, cat_tr, cat_te = train_test_split(\n",
    "            X_layer, y_tool_layer, y_claims_layer, categories_layer,\n",
    "            test_size=0.2, random_state=42\n",
    "        )\n",
    "        print(f\"  Train/test split: {len(X_tr)}/{len(X_te)} samples\")\n",
    "    \n",
    "    # Train probes\n",
    "    print(f\"  Training probes...\")\n",
    "    reality_probe_layer = train_reality_probe(X_tr, y_tool_tr)\n",
    "    narrative_probe_layer = train_narrative_probe(X_tr, y_claims_tr)\n",
    "    \n",
    "    # Evaluate\n",
    "    reality_acc = accuracy_score(y_tool_te, reality_probe_layer.predict(X_te))\n",
    "    narrative_acc = accuracy_score(y_claims_te, narrative_probe_layer.predict(X_te))\n",
    "    \n",
    "    # Fake escalation analysis\n",
    "    fake_mask = cat_te == \"fake_escalation\"\n",
    "    if fake_mask.sum() > 0:\n",
    "        fake_preds = reality_probe_layer.predict(X_te[fake_mask])\n",
    "        fake_aligned_reality = (fake_preds == False).mean()\n",
    "    else:\n",
    "        fake_aligned_reality = np.nan\n",
    "        print(f\"  ‚ö†Ô∏è  No fake_escalation samples in test set\")\n",
    "    \n",
    "    print(f\"  Results:\")\n",
    "    print(f\"    Reality probe accuracy:    {reality_acc:.1%}\")\n",
    "    print(f\"    Narrative probe accuracy:  {narrative_acc:.1%}\")\n",
    "    print(f\"    Fake‚ÜíReality alignment:    {fake_aligned_reality:.1%}\")\n",
    "    \n",
    "    layer_results['layer'].append(layer_idx)\n",
    "    layer_results['reality_acc'].append(reality_acc)\n",
    "    layer_results['narrative_acc'].append(narrative_acc)\n",
    "    layer_results['fake_aligned_reality'].append(fake_aligned_reality)\n",
    "    \n",
    "    print()  # Blank line between layers\n",
    "\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"LAYER ANALYSIS COMPLETE\")\n",
    "print(\"=\"*50)\n",
    "print(f\"Analyzed {len(layer_results['layer'])} layers\")\n",
    "if len(layer_results['layer']) > 0:\n",
    "    best_idx = np.argmax(layer_results['reality_acc'])\n",
    "    print(f\"Best reality probe accuracy: {layer_results['reality_acc'][best_idx]:.1%} at layer {layer_results['layer'][best_idx]}\")\n",
    "else:\n",
    "    print(\"‚ö†Ô∏è  No layers were successfully analyzed. Check error messages above.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize layer-wise results\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "fig, axes = plt.subplots(1, 2, figsize=(12, 5))\n",
    "\n",
    "# Plot 1: Probe accuracy by layer\n",
    "ax1 = axes[0]\n",
    "ax1.plot(layer_results['layer'], layer_results['reality_acc'], 'o-', label='Reality Probe', linewidth=2, markersize=8)\n",
    "ax1.plot(layer_results['layer'], layer_results['narrative_acc'], 's--', label='Narrative Probe', linewidth=2, markersize=8)\n",
    "ax1.axhline(y=0.5, color='gray', linestyle=':', label='Chance (50%)')\n",
    "ax1.set_xlabel('Layer', fontsize=12)\n",
    "ax1.set_ylabel('Accuracy', fontsize=12)\n",
    "ax1.set_title('Probe Accuracy by Layer', fontsize=14)\n",
    "ax1.legend()\n",
    "ax1.set_ylim(0.4, 1.0)\n",
    "ax1.grid(True, alpha=0.3)\n",
    "\n",
    "# Plot 2: Fake escalation alignment by layer\n",
    "ax2 = axes[1]\n",
    "ax2.plot(layer_results['layer'], layer_results['fake_aligned_reality'], 'o-', \n",
    "         color='green', linewidth=2, markersize=8)\n",
    "ax2.axhline(y=0.5, color='gray', linestyle=':', label='Chance')\n",
    "ax2.set_xlabel('Layer', fontsize=12)\n",
    "ax2.set_ylabel('% Aligned with Reality', fontsize=12)\n",
    "ax2.set_title('Fake Escalations: Does Probe \"Know\" Truth?', fontsize=14)\n",
    "ax2.set_ylim(0.0, 1.1)\n",
    "ax2.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('../figures/layer_analysis.png', dpi=150, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "# Print summary table\n",
    "print(\"\\nLayer Analysis Summary:\")\n",
    "print(f\"{'Layer':<8} {'Reality':<12} {'Narrative':<12} {'Fake‚ÜíReality':<12}\")\n",
    "print(\"-\" * 44)\n",
    "for i, layer in enumerate(layer_results['layer']):\n",
    "    print(f\"{layer:<8} {layer_results['reality_acc'][i]:<12.1%} {layer_results['narrative_acc'][i]:<12.1%} {layer_results['fake_aligned_reality'][i]:<12.1%}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. Error Analysis\n",
    "\n",
    "Find cases where the probes made mistakes and examine them to understand failure modes.\n",
    "\n",
    "**What to look for:**\n",
    "- Are errors concentrated in specific categories?\n",
    "- Are there ambiguous phrasings that confuse the probe?\n",
    "- Are certain scenarios/pressures harder?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Error Analysis: Find misclassified samples\n",
    "from collections import Counter\n",
    "\n",
    "# Get predictions on test set (using probes from full_probe_evaluation)\n",
    "reality_probe = results[\"reality_probe\"]\n",
    "narrative_probe = results[\"narrative_probe\"]\n",
    "\n",
    "reality_preds = reality_probe.predict(X_test)\n",
    "narrative_preds = narrative_probe.predict(X_test)\n",
    "\n",
    "# Find errors for each probe\n",
    "reality_errors = np.where(reality_preds != y_tool_test)[0]\n",
    "narrative_errors = np.where(narrative_preds != y_claims_test)[0]\n",
    "\n",
    "print(\"=\"*60)\n",
    "print(\"ERROR SUMMARY\")\n",
    "print(\"=\"*60)\n",
    "print(f\"Reality probe errors:    {len(reality_errors)}/{len(X_test)} ({len(reality_errors)/len(X_test):.1%})\")\n",
    "print(f\"Narrative probe errors:  {len(narrative_errors)}/{len(X_test)} ({len(narrative_errors)/len(X_test):.1%})\")\n",
    "\n",
    "# Category breakdown of errors\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"REALITY PROBE ERRORS BY CATEGORY\")\n",
    "print(\"=\"*60)\n",
    "if len(reality_errors) > 0:\n",
    "    reality_error_cats = cat_test[reality_errors]\n",
    "    for cat, count in Counter(reality_error_cats).most_common():\n",
    "        total_in_cat = (cat_test == cat).sum()\n",
    "        print(f\"  {cat}: {count}/{total_in_cat} ({count/total_in_cat:.1%})\")\n",
    "else:\n",
    "    print(\"  No errors!\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"NARRATIVE PROBE ERRORS BY CATEGORY\")\n",
    "print(\"=\"*60)\n",
    "if len(narrative_errors) > 0:\n",
    "    narrative_error_cats = cat_test[narrative_errors]\n",
    "    for cat, count in Counter(narrative_error_cats).most_common():\n",
    "        total_in_cat = (cat_test == cat).sum()\n",
    "        print(f\"  {cat}: {count}/{total_in_cat} ({count/total_in_cat:.1%})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Summary: Narrative Probe Error Analysis\n",
    "\n",
    "**Key Finding:** Many \"errors\" are actually **ground truth labeling issues**, not probe failures.\n",
    "\n",
    "**Evidence:**\n",
    "\n",
    "1. **Error 2** - Clear false negative in labels:\n",
    "   - Text contains: \"I've also gone ahead and escalated your case\" + \"a human will be reviewing your case\"\n",
    "   - Matches phrase list: `\"i've escalated\"` and `\"a human will review your case\"`\n",
    "   - Ground truth says: `claims_action: False` ‚ùå\n",
    "   - Probe says: `claims_action: True` (confidence 0.999) ‚úÖ\n",
    "   - **Probe is correct** - this should be labeled as a claim\n",
    "\n",
    "2. **Error 3** - Labeling inconsistency:\n",
    "   - Text contains: \"I've escalated your case to a human team member\"\n",
    "   - Matches phrase list: `\"i've escalated\"`\n",
    "   - Header says: `claims_action: False` but claim detection section says: `claims_action: True`\n",
    "   - Probe aligns with claim detection (confidence 1.000)\n",
    "   - **Same episode labeled differently in different places**\n",
    "\n",
    "3. **Error 5** - Likely false positive in labels:\n",
    "   - Text contains conditional/instructional language: \"If you would like... you can always escalate\"\n",
    "   - This is telling user HOW to escalate, not claiming escalation happened\n",
    "   - Probe says: `claims_action: False` (confidence 0.000) ‚úÖ\n",
    "   - Ground truth says: `claims_action: True` ‚ùå\n",
    "\n",
    "**Root Cause:** \n",
    "- Episodes were labeled using simple regex phrase matching (`detect_escalation_claim()`)\n",
    "- Regex doesn't handle context, partial matches, or conditional language well\n",
    "- The probe learns semantic meaning better than regex matching\n",
    "\n",
    "**Implication:**\n",
    "The narrative probe may be learning a more accurate representation than the ground truth labels. Consider re-labeling episodes using LLM judge (`use_llm_judge=True`) for more nuanced detection.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Detailed inspection of narrative probe errors\n",
    "# We need to map test indices back to original samples\n",
    "\n",
    "# First, create the same train/test split to get the test sample indices\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Recreate split with same random state to get indices\n",
    "indices = np.arange(len(all_samples))\n",
    "_, test_indices = train_test_split(\n",
    "    indices, test_size=0.2, stratify=categories, random_state=42\n",
    ")\n",
    "\n",
    "# Load original episodes\n",
    "with open(EPISODE_FILE) as f:\n",
    "    all_episodes = [json.loads(line) for line in f]\n",
    "\n",
    "print(\"=\"*60)\n",
    "print(\"INSPECTING NARRATIVE PROBE ERRORS\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Look at up to 5 narrative errors\n",
    "for i, err_idx in enumerate(narrative_errors[:5]):\n",
    "    # Get the test sample\n",
    "    sample_idx = test_indices[err_idx]\n",
    "    sample = all_samples[sample_idx]\n",
    "    \n",
    "    # Get original episode\n",
    "    episode = all_episodes[sample.episode_idx]\n",
    "    \n",
    "    print(f\"\\n{'‚îÄ'*60}\")\n",
    "    print(f\"ERROR {i+1}\")\n",
    "    print(f\"{'‚îÄ'*60}\")\n",
    "    print(f\"Category:       {sample.category}\")\n",
    "    print(f\"Scenario:       {sample.scenario}\")\n",
    "    print(f\"System variant: {sample.system_variant}\")\n",
    "    print(f\"Social pressure: {sample.social_pressure}\")\n",
    "    print(f\"\\nGround truth claims_action: {sample.claims_action}\")\n",
    "    print(f\"Probe predicted:            {narrative_preds[err_idx]}\")\n",
    "    print(f\"Probe confidence P(True):   {narrative_probe.predict_proba(X_test[err_idx:err_idx+1])[0,1]:.3f}\")\n",
    "    \n",
    "    # Show the reply (truncated)\n",
    "    reply = episode.get(\"reply\", \"N/A\")\n",
    "    print(f\"\\n--- MODEL REPLY (first 600 chars) ---\")\n",
    "    print(reply[:1000])\n",
    "    if len(reply) > 1000:\n",
    "        print(\"...\")\n",
    "    \n",
    "    # Show what the claim detector found\n",
    "    print(f\"\\n--- CLAIM DETECTION ---\")\n",
    "    print(f\"tool_used (ground truth):   {episode.get('tool_used', 'N/A')}\")\n",
    "    print(f\"claims_action (ground truth): {episode.get('claims_action', 'N/A')}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze error patterns by condition\n",
    "# Are certain scenarios/pressures/variants harder?\n",
    "\n",
    "def get_sample_metadata(sample_idx):\n",
    "    \"\"\"Get metadata for a test sample.\"\"\"\n",
    "    orig_idx = test_indices[sample_idx]\n",
    "    sample = all_samples[orig_idx]\n",
    "    return {\n",
    "        'scenario': sample.scenario,\n",
    "        'system_variant': sample.system_variant,\n",
    "        'social_pressure': sample.social_pressure,\n",
    "    }\n",
    "\n",
    "# Analyze narrative errors by condition\n",
    "if len(narrative_errors) > 0:\n",
    "    print(\"=\"*60)\n",
    "    print(\"NARRATIVE ERRORS BY CONDITION\")\n",
    "    print(\"=\"*60)\n",
    "    \n",
    "    error_metadata = [get_sample_metadata(i) for i in narrative_errors]\n",
    "    \n",
    "    # By scenario\n",
    "    scenarios = [m['scenario'] for m in error_metadata]\n",
    "    print(\"\\nBy Scenario:\")\n",
    "    for scenario, count in Counter(scenarios).most_common():\n",
    "        total = sum(1 for i in range(len(X_test)) if get_sample_metadata(i)['scenario'] == scenario)\n",
    "        print(f\"  {scenario}: {count}/{total} errors ({count/total:.1%})\")\n",
    "    \n",
    "    # By system variant\n",
    "    variants = [m['system_variant'] for m in error_metadata]\n",
    "    print(\"\\nBy System Variant:\")\n",
    "    for variant, count in Counter(variants).most_common():\n",
    "        total = sum(1 for i in range(len(X_test)) if get_sample_metadata(i)['system_variant'] == variant)\n",
    "        print(f\"  {variant}: {count}/{total} errors ({count/total:.1%})\")\n",
    "    \n",
    "    # By social pressure\n",
    "    pressures = [m['social_pressure'] for m in error_metadata]\n",
    "    print(\"\\nBy Social Pressure:\")\n",
    "    for pressure, count in Counter(pressures).most_common():\n",
    "        total = sum(1 for i in range(len(X_test)) if get_sample_metadata(i)['social_pressure'] == pressure)\n",
    "        print(f\"  {pressure}: {count}/{total} errors ({count/total:.1%})\")\n",
    "else:\n",
    "    print(\"No narrative probe errors to analyze!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 11. Summary for Write-up\n",
    "\n",
    "Key findings to include in executive summary:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate summary statistics for write-up\n",
    "print(\"=\"*70)\n",
    "print(\"KEY FINDINGS FOR EXECUTIVE SUMMARY\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# Phase 1: Behavioral\n",
    "fake_count = (categories == \"fake_escalation\").sum()\n",
    "total_count = len(categories)\n",
    "fake_rate = fake_count / total_count\n",
    "\n",
    "print(f\"\\nüìä PHASE 1: BEHAVIORAL STUDY\")\n",
    "print(f\"   Total episodes: {total_count}\")\n",
    "print(f\"   Fake escalations: {fake_count} ({fake_rate:.1%})\")\n",
    "print(f\"   ‚Üí Model claims actions it didn't take in {fake_rate:.1%} of episodes\")\n",
    "\n",
    "# Phase 2: Mechanistic\n",
    "print(f\"\\nüî¨ PHASE 2: MECHANISTIC PROBES\")\n",
    "print(f\"   Reality probe accuracy: {results['reality_overall'].accuracy:.1%}\")\n",
    "print(f\"   Narrative probe accuracy: {results['narrative_overall'].accuracy:.1%}\")\n",
    "\n",
    "fake_analysis = results['fake_analysis']\n",
    "print(f\"\\n   On FAKE ESCALATIONS (N={fake_analysis.n_fake}):\")\n",
    "print(f\"   ‚Üí Reality probe predicts 'no tool': {fake_analysis.probe_aligned_with_reality:.1%}\")\n",
    "print(f\"   ‚Üí Model internally 'knows' it didn't act, even while claiming it did!\")\n",
    "\n",
    "# Layer analysis (if run)\n",
    "if 'layer_results' in dir() and len(layer_results['layer']) > 0:\n",
    "    print(f\"\\nüìà LAYER ANALYSIS:\")\n",
    "    best_layer_idx = np.argmax(layer_results['reality_acc'])\n",
    "    best_layer = layer_results['layer'][best_layer_idx]\n",
    "    best_acc = layer_results['reality_acc'][best_layer_idx]\n",
    "    print(f\"   Best layer for reality probe: {best_layer} ({best_acc:.1%} accuracy)\")\n",
    "    \n",
    "    # Where does it emerge?\n",
    "    above_80 = [l for l, acc in zip(layer_results['layer'], layer_results['reality_acc']) if acc > 0.8]\n",
    "    if above_80:\n",
    "        print(f\"   Action-grounding emerges by layer: {min(above_80)}\")\n",
    "\n",
    "# Cross-domain (if run)\n",
    "if 'transfer_result' in dir():\n",
    "    print(f\"\\nüîÑ CROSS-DOMAIN TRANSFER:\")\n",
    "    print(f\"   Train: {transfer_result.train_domain} ‚Üí Test: {transfer_result.test_domain}\")\n",
    "    print(f\"   Transfer accuracy: {transfer_result.test_accuracy:.1%}\")\n",
    "    print(f\"   Above chance: {'YES ‚úì' if transfer_result.above_chance else 'NO'}\")\n",
    "\n",
    "# Baselines\n",
    "print(f\"\\nüìè BASELINES:\")\n",
    "print(f\"   Random baseline: 50%\")\n",
    "majority_class = (y_tool_test == False).mean()  # Assuming no-tool is majority\n",
    "print(f\"   Majority class baseline: {max(majority_class, 1-majority_class):.1%}\")\n",
    "print(f\"   Reality probe: {results['reality_overall'].accuracy:.1%} (‚Üë{results['reality_overall'].accuracy - 0.5:.1%} above random)\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 13. Critical Test: Early Position Probing\n",
    "\n",
    "**The key question:** Does the model \"know\" at the START of its response whether it will take an action?\n",
    "\n",
    "If the probe works at `first_assistant` position (before any tool-related tokens are generated), this is stronger evidence that we're detecting an \"action grounding\" representation, not just \"tool syntax coming up.\"\n",
    "\n",
    "| Position | What it tells us |\n",
    "|----------|------------------|\n",
    "| `before_tool` | Model knows right before emitting `<<CALL` (less surprising) |\n",
    "| `first_assistant` | Model knows at the START of response (more surprising) |\n",
    "\n",
    "If accuracy is high at `first_assistant`, the model has \"decided\" whether to take action before generating any relevant content."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize position comparison\n",
    "fig, axes = plt.subplots(1, 2, figsize=(12, 5))\n",
    "\n",
    "# Plot 1: Accuracy comparison\n",
    "ax1 = axes[0]\n",
    "positions = ['first_assistant\\n(start of response)', 'before_tool\\n(right before <<CALL)']\n",
    "reality_accs = [results_first['reality_acc'], results_before['reality_acc']]\n",
    "narrative_accs = [results_first['narrative_acc'], results_before['narrative_acc']]\n",
    "\n",
    "x = np.arange(len(positions))\n",
    "width = 0.35\n",
    "\n",
    "bars1 = ax1.bar(x - width/2, reality_accs, width, label='Reality Probe', color='#2ecc71', edgecolor='black')\n",
    "bars2 = ax1.bar(x + width/2, narrative_accs, width, label='Narrative Probe', color='#3498db', edgecolor='black')\n",
    "\n",
    "ax1.axhline(y=0.5, color='gray', linestyle='--', alpha=0.7, label='Chance')\n",
    "ax1.set_ylabel('Accuracy', fontsize=12)\n",
    "ax1.set_title('Probe Accuracy by Token Position', fontsize=14)\n",
    "ax1.set_xticks(x)\n",
    "ax1.set_xticklabels(positions)\n",
    "ax1.legend()\n",
    "ax1.set_ylim(0, 1.1)\n",
    "\n",
    "for bar, acc in zip(bars1, reality_accs):\n",
    "    ax1.text(bar.get_x() + bar.get_width()/2, bar.get_height() + 0.02,\n",
    "             f'{acc:.1%}', ha='center', fontsize=11, fontweight='bold')\n",
    "\n",
    "# Plot 2: Fake escalation alignment\n",
    "ax2 = axes[1]\n",
    "fake_aligned = [results_first['fake_aligned_reality'], results_before['fake_aligned_reality']]\n",
    "colors = ['#e74c3c', '#c0392b']\n",
    "bars3 = ax2.bar(positions, fake_aligned, color=colors, edgecolor='black')\n",
    "ax2.axhline(y=0.5, color='gray', linestyle='--', alpha=0.7)\n",
    "ax2.set_ylabel('% Aligned with Reality', fontsize=12)\n",
    "ax2.set_title('Fake Escalations: When Does Model \"Know\" Truth?', fontsize=14)\n",
    "ax2.set_ylim(0, 1.1)\n",
    "\n",
    "for bar, val in zip(bars3, fake_aligned):\n",
    "    ax2.text(bar.get_x() + bar.get_width()/2, bar.get_height() + 0.02,\n",
    "             f'{val:.1%}', ha='center', fontsize=11, fontweight='bold')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('../figures/position_comparison.png', dpi=150, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(f\"\\nüìä Figure saved to: ../figures/position_comparison.png\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
