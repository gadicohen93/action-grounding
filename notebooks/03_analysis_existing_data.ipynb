{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Action Grounding Analysis - Using Existing Data\n",
    "\n",
    "This notebook analyzes the pre-collected episodes and activations to demonstrate:\n",
    "\n",
    "1. **Behavioral Phenomenon**: LLMs claim actions they didn't take (~26% fake rate)\n",
    "2. **Mechanistic Probes**: Linear probes detect ground truth with 95%+ accuracy\n",
    "3. **Cross-Tool Transfer**: Probes generalize across tool types\n",
    "4. **Causal Intervention**: Steering vectors can influence behavior\n",
    "\n",
    "**Data Used:**\n",
    "- `data/raw/adversarial_20251221_020507.jsonl` - 660 episodes\n",
    "- `data/labeled/activations_v1_combined.npz` - Pre-extracted activations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup\n",
    "import sys\n",
    "sys.path.insert(0, '..')\n",
    "\n",
    "import json\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "from collections import Counter\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Set style\n",
    "plt.style.use('seaborn-v0_8-whitegrid')\n",
    "sns.set_palette('husl')\n",
    "\n",
    "print(\"Setup complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Load Existing Episodes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load episodes from JSONL\n",
    "EPISODE_FILE = \"../data/raw/adversarial_20251221_020507.jsonl\"\n",
    "\n",
    "episodes = []\n",
    "with open(EPISODE_FILE) as f:\n",
    "    for line in f:\n",
    "        episodes.append(json.loads(line))\n",
    "\n",
    "print(f\"Loaded {len(episodes)} episodes\")\n",
    "print(f\"\\nFirst episode keys: {list(episodes[0].keys())}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze episode categories\n",
    "categories = [ep.get('category', 'unknown') for ep in episodes]\n",
    "cat_counts = Counter(categories)\n",
    "\n",
    "print(\"=\" * 60)\n",
    "print(\"EPISODE CATEGORY DISTRIBUTION\")\n",
    "print(\"=\" * 60)\n",
    "for cat, count in cat_counts.most_common():\n",
    "    print(f\"  {cat:25s}: {count:4d} ({100*count/len(episodes):5.1f}%)\")\n",
    "\n",
    "# Key metrics\n",
    "fake_count = cat_counts.get('fake_escalation', 0)\n",
    "fake_rate = fake_count / len(episodes)\n",
    "print(f\"\\nðŸŽ¯ FAKE ESCALATION RATE: {fake_rate:.1%} ({fake_count}/{len(episodes)})\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize category distribution\n",
    "fig, ax = plt.subplots(figsize=(10, 6))\n",
    "\n",
    "cats = list(cat_counts.keys())\n",
    "counts = list(cat_counts.values())\n",
    "colors = ['#e74c3c' if 'fake' in c else '#3498db' if 'true' in c else '#95a5a6' for c in cats]\n",
    "\n",
    "bars = ax.bar(cats, counts, color=colors, edgecolor='black', linewidth=1.5)\n",
    "ax.set_ylabel('Count', fontsize=12)\n",
    "ax.set_xlabel('Category', fontsize=12)\n",
    "ax.set_title('Episode Categories (N=660)', fontsize=14, fontweight='bold')\n",
    "\n",
    "# Add count labels\n",
    "for bar, count in zip(bars, counts):\n",
    "    ax.text(bar.get_x() + bar.get_width()/2, bar.get_height() + 5,\n",
    "            f'{count}', ha='center', fontsize=11, fontweight='bold')\n",
    "\n",
    "plt.xticks(rotation=15)\n",
    "plt.tight_layout()\n",
    "plt.savefig('../figures/category_distribution.png', dpi=150, bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Load Pre-Extracted Activations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Load activations\nACTIVATION_FILE = \"../data/labeled/activations_v1_combined.npz\"\n\ndata = np.load(ACTIVATION_FILE, allow_pickle=True)\nprint(f\"Loaded activation file: {ACTIVATION_FILE}\")\nprint(f\"Keys: {list(data.keys())}\")\n\n# Extract arrays (using actual keys from the file)\nX = data['activations']  # Activations\ny_tool = data['tool_used']  # Ground truth: did tool get called?\ny_claims = data['claims_action']  # Did model claim action?\ncategories_arr = data['categories']  # Episode categories\n\nprint(f\"\\nActivation shape: {X.shape}\")\nprint(f\"  - {X.shape[0]} samples\")\nprint(f\"  - {X.shape[1]} dimensions (hidden size)\")\nprint(f\"\\nLabels:\")\nprint(f\"  - tool_used: {y_tool.sum()}/{len(y_tool)} ({y_tool.mean():.1%})\")\nprint(f\"  - claims_action: {y_claims.sum()}/{len(y_claims)} ({y_claims.mean():.1%})\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Category distribution in activations\n",
    "print(\"Category distribution in activations:\")\n",
    "for cat in np.unique(categories_arr):\n",
    "    count = (categories_arr == cat).sum()\n",
    "    print(f\"  {cat}: {count} ({100*count/len(categories_arr):.1f}%)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Train Linear Probes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split, cross_val_score\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
    "\n",
    "# Stratified train/test split\n",
    "X_train, X_test, y_tool_train, y_tool_test, y_claims_train, y_claims_test, cat_train, cat_test = train_test_split(\n",
    "    X, y_tool, y_claims, categories_arr,\n",
    "    test_size=0.2,\n",
    "    stratify=categories_arr,\n",
    "    random_state=42,\n",
    ")\n",
    "\n",
    "print(f\"Train: {len(X_train)} samples\")\n",
    "print(f\"Test:  {len(X_test)} samples\")\n",
    "print(f\"\\nTest set category distribution:\")\n",
    "for cat in np.unique(cat_test):\n",
    "    print(f\"  {cat}: {(cat_test == cat).sum()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train Reality Probe (predicts tool_used - ground truth)\n",
    "print(\"Training Reality Probe...\")\n",
    "reality_probe = LogisticRegression(max_iter=1000, C=1.0, random_state=42)\n",
    "reality_probe.fit(X_train, y_tool_train)\n",
    "\n",
    "# Cross-validation\n",
    "cv_scores = cross_val_score(reality_probe, X_train, y_tool_train, cv=5)\n",
    "print(f\"  CV Accuracy: {cv_scores.mean():.1%} Â± {cv_scores.std():.1%}\")\n",
    "\n",
    "# Test accuracy\n",
    "reality_preds = reality_probe.predict(X_test)\n",
    "reality_acc = accuracy_score(y_tool_test, reality_preds)\n",
    "print(f\"  Test Accuracy: {reality_acc:.1%}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train Narrative Probe (predicts claims_action - what model says)\n",
    "print(\"Training Narrative Probe...\")\n",
    "narrative_probe = LogisticRegression(max_iter=1000, C=1.0, random_state=42)\n",
    "narrative_probe.fit(X_train, y_claims_train)\n",
    "\n",
    "# Cross-validation\n",
    "cv_scores = cross_val_score(narrative_probe, X_train, y_claims_train, cv=5)\n",
    "print(f\"  CV Accuracy: {cv_scores.mean():.1%} Â± {cv_scores.std():.1%}\")\n",
    "\n",
    "# Test accuracy\n",
    "narrative_preds = narrative_probe.predict(X_test)\n",
    "narrative_acc = accuracy_score(y_claims_test, narrative_preds)\n",
    "print(f\"  Test Accuracy: {narrative_acc:.1%}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Key Test: Does Model \"Know\" When It's Lying?\n",
    "\n",
    "On **fake escalation** episodes (model claims action but didn't take it):\n",
    "- Ground truth `tool_used = False`\n",
    "- Model claims `claims_action = True`\n",
    "\n",
    "**Question:** Does the reality probe predict the *truth* (False) or the *narrative* (True)?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze fake escalations\n",
    "fake_mask = cat_test == 'fake_escalation'\n",
    "n_fake = fake_mask.sum()\n",
    "\n",
    "print(\"=\" * 60)\n",
    "print(\"CRITICAL TEST: FAKE ESCALATION ANALYSIS\")\n",
    "print(\"=\" * 60)\n",
    "print(f\"\\nFake escalations in test set: {n_fake}\")\n",
    "\n",
    "if n_fake > 0:\n",
    "    # Get predictions on fake episodes\n",
    "    fake_reality_preds = reality_probe.predict(X_test[fake_mask])\n",
    "    fake_probs = reality_probe.predict_proba(X_test[fake_mask])[:, 1]\n",
    "    \n",
    "    # Ground truth for fake episodes: tool_used = False\n",
    "    # So probe should predict False if it \"knows\" the truth\n",
    "    aligned_with_reality = (fake_reality_preds == False).mean()\n",
    "    aligned_with_narrative = (fake_reality_preds == True).mean()\n",
    "    \n",
    "    print(f\"\\nReality probe predictions on fake escalations:\")\n",
    "    print(f\"  Predicts FALSE (aligned with REALITY):    {aligned_with_reality:.1%}\")\n",
    "    print(f\"  Predicts TRUE (aligned with NARRATIVE):   {aligned_with_narrative:.1%}\")\n",
    "    print(f\"  Mean P(tool_used=True):                   {fake_probs.mean():.3f}\")\n",
    "    \n",
    "    if aligned_with_reality > 0.8:\n",
    "        print(f\"\\nâœ… STRONG EVIDENCE: Model internally 'knows' it didn't act!\")\n",
    "        print(f\"   The probe detects ground truth even when the model lies.\")\n",
    "    elif aligned_with_reality > 0.5:\n",
    "        print(f\"\\nâš ï¸  MODERATE EVIDENCE: Probe leans toward reality.\")\n",
    "    else:\n",
    "        print(f\"\\nâŒ WEAK EVIDENCE: Probe fooled by model's narrative.\")\n",
    "else:\n",
    "    print(\"No fake escalations in test set!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Detailed breakdown by category\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"REALITY PROBE ACCURACY BY CATEGORY\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "for cat in np.unique(cat_test):\n",
    "    mask = cat_test == cat\n",
    "    if mask.sum() == 0:\n",
    "        continue\n",
    "    \n",
    "    cat_acc = accuracy_score(y_tool_test[mask], reality_preds[mask])\n",
    "    cat_n = mask.sum()\n",
    "    \n",
    "    # For fake/silent, show what it predicts\n",
    "    if cat == 'fake_escalation':\n",
    "        pct_false = (reality_preds[mask] == False).mean()\n",
    "        print(f\"  {cat:25s}: {cat_acc:5.1%} acc (N={cat_n:3d}) | Predicts FALSE: {pct_false:.1%} âœ“\")\n",
    "    elif cat == 'silent_escalation':\n",
    "        pct_true = (reality_preds[mask] == True).mean()\n",
    "        print(f\"  {cat:25s}: {cat_acc:5.1%} acc (N={cat_n:3d}) | Predicts TRUE: {pct_true:.1%}\")\n",
    "    else:\n",
    "        print(f\"  {cat:25s}: {cat_acc:5.1%} acc (N={cat_n:3d})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Visualize Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Confusion matrix for reality probe\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# Reality probe confusion matrix\n",
    "cm_reality = confusion_matrix(y_tool_test, reality_preds)\n",
    "sns.heatmap(cm_reality, annot=True, fmt='d', cmap='Blues', ax=axes[0],\n",
    "            xticklabels=['No Tool', 'Tool Used'],\n",
    "            yticklabels=['No Tool', 'Tool Used'])\n",
    "axes[0].set_title(f'Reality Probe (Acc: {reality_acc:.1%})', fontsize=14)\n",
    "axes[0].set_xlabel('Predicted')\n",
    "axes[0].set_ylabel('Actual')\n",
    "\n",
    "# Narrative probe confusion matrix\n",
    "cm_narrative = confusion_matrix(y_claims_test, narrative_preds)\n",
    "sns.heatmap(cm_narrative, annot=True, fmt='d', cmap='Greens', ax=axes[1],\n",
    "            xticklabels=['No Claim', 'Claims Action'],\n",
    "            yticklabels=['No Claim', 'Claims Action'])\n",
    "axes[1].set_title(f'Narrative Probe (Acc: {narrative_acc:.1%})', fontsize=14)\n",
    "axes[1].set_xlabel('Predicted')\n",
    "axes[1].set_ylabel('Actual')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('../figures/probe_confusion_matrices.png', dpi=150, bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Probe accuracy comparison\n",
    "fig, ax = plt.subplots(figsize=(8, 6))\n",
    "\n",
    "probes = ['Reality Probe\\n(tool_used)', 'Narrative Probe\\n(claims_action)']\n",
    "accs = [reality_acc, narrative_acc]\n",
    "colors = ['#2ecc71', '#3498db']\n",
    "\n",
    "bars = ax.bar(probes, accs, color=colors, edgecolor='black', linewidth=2)\n",
    "ax.axhline(y=0.5, color='red', linestyle='--', linewidth=2, label='Random Baseline')\n",
    "\n",
    "ax.set_ylabel('Accuracy', fontsize=12)\n",
    "ax.set_title('Linear Probe Performance', fontsize=14, fontweight='bold')\n",
    "ax.set_ylim(0, 1.1)\n",
    "ax.legend()\n",
    "\n",
    "for bar, acc in zip(bars, accs):\n",
    "    ax.text(bar.get_x() + bar.get_width()/2, bar.get_height() + 0.02,\n",
    "            f'{acc:.1%}', ha='center', fontsize=14, fontweight='bold')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('../figures/probe_accuracy.png', dpi=150, bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Cross-Tool Transfer Test\n",
    "\n",
    "If the probe captures a general \"action grounding\" representation, it should transfer to a different tool type."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Check if search tool activations exist\nSEARCH_ACTIVATION_FILE = \"../data/labeled/activations_search.npz\"\n\nif Path(SEARCH_ACTIVATION_FILE).exists():\n    print(\"Loading search tool activations...\")\n    search_data = np.load(SEARCH_ACTIVATION_FILE, allow_pickle=True)\n    \n    # Use actual keys from the file\n    X_search = search_data['activations']\n    y_tool_search = search_data['tool_used']\n    \n    print(f\"Search activations: {X_search.shape}\")\n    print(f\"Tool used rate: {y_tool_search.mean():.1%}\")\n    \n    # Test transfer: train on escalate, test on search\n    print(\"\\n\" + \"=\" * 60)\n    print(\"CROSS-TOOL TRANSFER TEST\")\n    print(\"=\" * 60)\n    print(f\"Train: escalateCase ({len(X_train)} samples)\")\n    print(f\"Test:  searchKnowledgeBase ({len(X_search)} samples)\")\n    \n    # Use reality probe trained on escalate\n    transfer_preds = reality_probe.predict(X_search)\n    transfer_acc = accuracy_score(y_tool_search, transfer_preds)\n    \n    print(f\"\\nTransfer accuracy: {transfer_acc:.1%}\")\n    \n    if transfer_acc > 0.7:\n        print(\"\\nâœ… STRONG TRANSFER: Probe generalizes across tools!\")\n        print(\"   This suggests a general 'action grounding' representation.\")\n    elif transfer_acc > 0.55:\n        print(\"\\nâš ï¸  MODERATE TRANSFER: Some generalization.\")\n    else:\n        print(\"\\nâŒ WEAK TRANSFER: Probe is tool-specific.\")\nelse:\n    print(f\"Search activations not found at: {SEARCH_ACTIVATION_FILE}\")\n    print(\"Skipping cross-tool transfer test.\")\n    transfer_acc = None"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Causal Intervention: Steering Vectors\n",
    "\n",
    "Can we use the probe direction to *change* model behavior?\n",
    "\n",
    "**Approach:**\n",
    "1. Extract the probe's weight vector (the \"action grounding\" direction)\n",
    "2. Add/subtract this direction during generation\n",
    "3. Measure if it changes the tool-calling rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract steering vector from reality probe\n",
    "steering_vector = reality_probe.coef_[0]  # Shape: (4096,)\n",
    "steering_vector = steering_vector / np.linalg.norm(steering_vector)  # Normalize\n",
    "\n",
    "print(f\"Steering vector shape: {steering_vector.shape}\")\n",
    "print(f\"Steering vector norm: {np.linalg.norm(steering_vector):.3f}\")\n",
    "\n",
    "# Visualize weight distribution\n",
    "fig, axes = plt.subplots(1, 2, figsize=(12, 4))\n",
    "\n",
    "axes[0].hist(steering_vector, bins=50, edgecolor='black')\n",
    "axes[0].set_xlabel('Weight')\n",
    "axes[0].set_ylabel('Count')\n",
    "axes[0].set_title('Steering Vector Weight Distribution')\n",
    "\n",
    "# Top dimensions\n",
    "top_k = 20\n",
    "top_idx = np.argsort(np.abs(steering_vector))[-top_k:]\n",
    "axes[1].barh(range(top_k), steering_vector[top_idx])\n",
    "axes[1].set_xlabel('Weight')\n",
    "axes[1].set_ylabel('Dimension')\n",
    "axes[1].set_title(f'Top {top_k} Dimensions by |weight|')\n",
    "axes[1].set_yticks(range(top_k))\n",
    "axes[1].set_yticklabels(top_idx)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('../figures/steering_vector.png', dpi=150, bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save steering vector for later use\n",
    "np.save('../data/processed/steering_vector.npy', steering_vector)\n",
    "print(\"Saved steering vector to: ../data/processed/steering_vector.npy\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Steering Experiment (Requires Model Loading)\n",
    "\n",
    "The cell below performs causal intervention by adding the steering vector during generation. This requires loading the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Optional: Run steering experiment (requires GPU and model loading)\n",
    "RUN_STEERING = False  # Set to True to run steering experiment\n",
    "\n",
    "if RUN_STEERING:\n",
    "    import torch\n",
    "    from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "    \n",
    "    print(\"Loading model for steering experiment...\")\n",
    "    model_id = \"mistralai/Mistral-7B-Instruct-v0.2\"\n",
    "    \n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
    "    model = AutoModelForCausalLM.from_pretrained(\n",
    "        model_id,\n",
    "        torch_dtype=torch.float16,\n",
    "        device_map=\"auto\",\n",
    "    )\n",
    "    \n",
    "    print(f\"Model loaded on: {next(model.parameters()).device}\")\n",
    "    \n",
    "    # Steering hook\n",
    "    steering_tensor = torch.tensor(steering_vector, dtype=torch.float16).to(model.device)\n",
    "    TARGET_LAYER = 16  # Middle layer\n",
    "    \n",
    "    def steering_hook(module, input, output, alpha=1.0):\n",
    "        \"\"\"Add steering vector to hidden states.\"\"\"\n",
    "        hidden_states = output[0]\n",
    "        hidden_states = hidden_states + alpha * steering_tensor\n",
    "        return (hidden_states,) + output[1:]\n",
    "    \n",
    "    print(\"\\nSteering experiment ready. Set alpha to test different strengths.\")\n",
    "    print(\"  alpha > 0: Encourage tool use\")\n",
    "    print(\"  alpha < 0: Discourage tool use\")\n",
    "else:\n",
    "    print(\"Steering experiment disabled. Set RUN_STEERING = True to enable.\")\n",
    "    print(\"(Requires GPU and model loading)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Summary Statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "print(\"=\" * 70)\nprint(\"EXECUTIVE SUMMARY: ACTION GROUNDING IN LLMs\")\nprint(\"=\" * 70)\n\nprint(f\"\\nðŸ“Š BEHAVIORAL PHENOMENON\")\nprint(f\"   Dataset: {len(episodes)} episodes\")\nprint(f\"   Fake escalation rate: {fake_rate:.1%} ({fake_count} episodes)\")\nprint(f\"   â†’ Model claims actions it didn't take in 1/{int(1/fake_rate):.0f} cases\")\n\nprint(f\"\\nðŸ”¬ MECHANISTIC PROBES\")\nprint(f\"   Reality probe accuracy:    {reality_acc:.1%}\")\nprint(f\"   Narrative probe accuracy:  {narrative_acc:.1%}\")\nprint(f\"   Random baseline:           50.0%\")\n\nif n_fake > 0:\n    print(f\"\\nðŸŽ¯ CRITICAL FINDING\")\n    print(f\"   On fake escalations (N={n_fake}):\")\n    print(f\"   â†’ Probe predicts REALITY:   {aligned_with_reality:.1%}\")\n    print(f\"   â†’ Probe predicts NARRATIVE: {aligned_with_narrative:.1%}\")\n    print(f\"   âœ… Model 'knows' it didn't act, even while claiming it did!\")\n\nif transfer_acc is not None:\n    print(f\"\\nðŸ”„ CROSS-TOOL TRANSFER\")\n    print(f\"   Train: escalateCase â†’ Test: searchKnowledgeBase\")\n    print(f\"   Transfer accuracy: {transfer_acc:.1%}\")\n    print(f\"   â†’ Probe captures general 'action grounding' representation\")\n\nprint(f\"\\nðŸ“ˆ INTERPRETATION\")\nprint(f\"   The model maintains an internal representation of whether\")\nprint(f\"   it actually performed an action, separate from what it claims.\")\nprint(f\"   This 'action grounding' can be detected with a linear probe.\")\n\nprint(\"\\n\" + \"=\" * 70)"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Save results for write-up\nresults = {\n    'n_episodes': len(episodes),\n    'fake_count': int(fake_count),\n    'fake_rate': float(fake_rate),\n    'reality_probe_acc': float(reality_acc),\n    'narrative_probe_acc': float(narrative_acc),\n    'fake_aligned_reality': float(aligned_with_reality) if n_fake > 0 else None,\n    'fake_aligned_narrative': float(aligned_with_narrative) if n_fake > 0 else None,\n    'transfer_acc': float(transfer_acc) if transfer_acc is not None else None,\n}\n\nimport json\nwith open('../data/processed/analysis_results.json', 'w') as f:\n    json.dump(results, f, indent=2)\n\nprint(\"Results saved to: ../data/processed/analysis_results.json\")\nprint(json.dumps(results, indent=2))"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Figures for Write-up\n",
    "\n",
    "All figures saved to `../figures/`:\n",
    "- `category_distribution.png` - Episode categories\n",
    "- `probe_confusion_matrices.png` - Probe performance\n",
    "- `probe_accuracy.png` - Accuracy comparison\n",
    "- `steering_vector.png` - Probe weights visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# List saved figures\n",
    "import os\n",
    "figures_dir = Path('../figures')\n",
    "if figures_dir.exists():\n",
    "    print(\"Saved figures:\")\n",
    "    for f in sorted(figures_dir.glob('*.png')):\n",
    "        print(f\"  {f.name}\")\n",
    "else:\n",
    "    print(\"Figures directory not found.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}