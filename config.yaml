# Action Grounding Research Configuration
# ========================================
# This file configures the research pipeline for studying
# whether LLMs maintain internal representations of action execution.
#
# FAST MODE: Set fast_mode: true to run entire pipeline in ~10 minutes
#            (uses fewer episodes but still gets meaningful results)

# ============================================================================
# QUICK START MODES
# ============================================================================
# fast_mode: true   -> ~10 min, 90 episodes (proof of concept)
# fast_mode: false  -> ~60 min, 450+ episodes (publication quality)
fast_mode: true

model:
  # HuggingFace model ID - must match for generation AND extraction
  id: "mistralai/Mistral-7B-Instruct-v0.2"
  # Backend: "pytorch" (stable, supports activation extraction) or "vllm" (faster but needs setup)
  # Note: vLLM requires: pip install vllm (large dependencies ~5GB)
  # If you get "MistralForCausalLM not found" errors, use "pytorch" backend instead
  backend: "pytorch"  # Stable choice; use "vllm" for 2-3x faster inference if it works
  # Quantization: "none", "4bit", "8bit"
  # Use "8bit" for most GPUs, "none" if you have 40GB+ VRAM
  quantization: "8bit"
  # Device mapping: "auto" for multi-GPU, "cuda:0" for single GPU
  device_map: "auto"
  # Model dtype: "float16" (default), "bfloat16" (A100+), "float32" (debug)
  dtype: "float16"

generation:
  temperature: 0.7
  max_tokens: 256  # Reduced from 512 for faster generation (increase for longer responses)
  top_p: 0.9
  # Fixed seed for reproducibility
  seed: 42

data:
  raw_dir: "./data/raw"
  processed_dir: "./data/processed"
  figures_dir: "./figures"

labeling:
  # Method for detecting action claims: "openai" (recommended), "regex", "local"
  method: "openai"
  # OpenAI model for LLM-based labeling
  openai_model: "gpt-4o-mini"
  batch_size: 20

experiment:
  # ============================================================================
  # EPISODE COUNTS (automatically adjusted by fast_mode)
  # ============================================================================
  # FAST MODE (fast_mode: true):
  #   - 2 episodes per condition = 90 total episodes
  #   - Good for debugging and proof-of-concept
  #   - Runs in ~10 minutes
  #
  # FULL MODE (fast_mode: false):
  #   - 10 episodes per condition = 450 total episodes
  #   - Good for meaningful statistical results
  #   - Runs in ~60 minutes
  #
  # PUBLICATION MODE: Set n_episodes_per_condition: 50
  #   - 50 per condition = 2,250 total episodes
  #   - Publication-quality results
  #   - Runs in ~4-6 hours
  # ============================================================================
  n_episodes_per_condition: 10  # Overridden by fast_mode if true
  n_episodes_per_condition_fast: 2  # Used when fast_mode: true

  # Tool types to test
  tools:
    - "escalate"
    - "search"
    - "sendMessage"
  # System prompt variants
  system_variants:
    - "A_STRICT"
    - "B_DILUTED"
    - "C_CONFLICTING"
  # Social pressure conditions
  social_pressures:
    - "NEUTRAL"
    - "STRESSED"
    - "DEMAND"
    - "VALIDATION"
    - "APPEASE"

extraction:
  # Token positions to extract activations from
  positions:
    - "first_assistant"
    - "mid_response"
    - "before_tool"
  # Layers to extract (0-indexed, Mistral has 32 layers)
  layers:
    - 0
    - 8
    - 16
    - 24
    - 31
  batch_size: 8

probe:
  # L2 regularization strength (C parameter for LogisticRegression)
  regularization: 1.0
  # Number of cross-validation folds
  n_folds: 5
  # Bootstrap samples for confidence intervals
  bootstrap_samples: 1000

steering:
  # Steering strengths to test (negative = subtract, positive = add)
  alphas:
    - -2.0
    - -1.0
    - -0.5
    - 0.0
    - 0.5
    - 1.0
    - 2.0
  # Samples per steering strength
  n_samples_per_alpha: 50
  # Layer to apply steering at (middle layers typically work best)
  target_layer: 16
